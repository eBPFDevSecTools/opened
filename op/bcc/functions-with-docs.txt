
int trace_req_done (struct pt_regs *ctx, struct request *req)
{
    dist.increment (bpf_log2l (req->__data_len / 1024));
    dist_linear.increment (req->__data_len / 1024);
    return 0;
}

"""
Summary of the above code:


1. We define a BPF_HISTOGRAM_SIZE macro to be the size of our histogram array.
2. We define a BPF_LOG_BUF_SIZE macro to be the size of our log buffer array.
3. We create two maps: one for the histogram and one for the log buffer, both with key type u32 and value type u64 (unsigned long). The histogram map is pinned in memory so that we can access it from user space later on, while the log buffer map is not pinned in memory because we don't need to access it from user


int hello (void *ctx)
{
    bpf_trace_printk ("Hello, World!\\n");
    return 0;
}

"""
Summary of the above code:


1. The first line of the code is a comment. Comments in C start with /* and end with */.
2. The next line includes the header file bpf_helpers.h, which contains various helper macros and functions that are needed to write eBPF programs in C (more on this later).
3. Next, we define a function called hello() that takes one argument of type void *ctx (more on this later) and returns an integer value 0 (which indicates success). This function prints “Hello, World!” using the bpf_trace_printk()


void trace_start (struct pt_regs *ctx, struct request *req)
{
    u64 ts = bpf_ktime_get_ns ();
    start.update (&req, &ts);
}

"""
Summary of the above code:


1. We define a map called start, which is of type BPF_MAP_TYPE_HASH. The key is the pointer to the request structure and the value is a 64-bit integer (timestamp).
2. We define a function called trace_start, which takes two arguments: ctx (pointer to pt_regs) and req (pointer to request). This function gets the current timestamp in nanoseconds using bpf_ktime_get_ns() and stores it in start map with req as key.
3. In this example, we are not interested in tracing


void trace_completion (struct pt_regs *ctx, struct request *req)
{
    u64 *tsp, delta;
    tsp = start.lookup (&req);
    if (tsp != 0) {
        delta = bpf_ktime_get_ns () - *tsp;
        bpf_trace_printk ("%d %x %d\\n", req->__data_len, req->cmd_flags, delta / 1000);
        start.delete (&req);
    }
}

"""
Summary of the above code:


1. The first function trace_start() is called when a request is submitted to the block layer. It stores the current time in a map, using the request pointer as key.
2. The second function trace_completion() is called when a request completes and it looks up the start time from the map and calculates how long it took for this request to complete (delta). It then prints out some information about this request, including its length (req->__data_len), flags (req->cmd_flags) and completion latency in microseconds (delta / 1000).
3. Finally, we


int count (struct pt_regs *ctx)
{
    dist.increment (bpf_log2l (PT_REGS_RC (ctx)));
    return 0;
}

"""
Summary of the above code:


1. The BPF program is defined as a C function called count, which takes a struct pt_regs *ctx as an argument.
2. The BPF program increments the value of the histogram keyed by bpf_log2l(PT_REGS_RC(ctx)).
3. The return value of 0 indicates that the kprobe should not be disabled after this probe handler returns (i.e., it should continue to fire).



void submit_impl_func_addr (struct pt_regs *ctx)
{
    u64 addr = PT_REGS_RC (ctx);
    impl_func_addr.perf_submit (ctx, &addr, sizeof (addr));
}

"""
Summary of the above code:


1. The function submit_impl_func_addr is called when the kernel encounters a breakpoint at the address of the function impl_func.
2. The value of register rcx is read and stored in addr variable.
3. The perf event handler for this probe is called with ctx (context) and addr as arguments, which are then passed to perf_submit() function defined in kernel/events/core.c file, which writes these values to a ring buffer that can be read by user space programs using perf tool or libpfm4 library functions like pfm_read().


The


int submit_resolv_func_addr (struct pt_regs *ctx)
{
    u64 rip = PT_REGS_IP (ctx);
    resolv_func_addr.perf_submit (ctx, &rip, sizeof (rip));
    return 0;
}

"""
Summary of the above code:


1. The resolv_func_addr tracepoint is defined in the kernel source code, and it is used to trace the address of a function that resolves a symbol.
2. The submit_resolv_func_addr() function is called when the resolv_func_addr tracepoint fires, and it submits data to perf for processing.
3. The perf probe command creates an event named "resolve" that uses the submit_resolv_func addr() function as its handler (see line 2).
4. When perf processes an event named "


int count (struct pt_regs *ctx)
{
    if (!PT_REGS_PARM1(ctx))
        return 0;
    struct key_t key = {}
    ;
    u64 zero = 0, *val;
    bpf_probe_read_user (&key.c, sizeof (key.c), (void *) PT_REGS_PARM1 (ctx));
    val = counts.lookup_or_try_init (&key, &zero);
    if (val) {
        (*val)++;
    }
    return 0;
}

"""
Summary of the above code:


1. The count function is called when the probe hits the kprobe.
2. The key_t struct is used to store the character that was passed as an argument to the sys_write function.
3. The counts map stores a count of each character that was passed as an argument to sys_write, and it uses key_t as its key type and u64 for its value type (the number of times each character was written).
4. When a new character is encountered, it’s added to the map with a value of 1; otherwise, its existing value in the map is


int printarg (struct pt_regs *ctx)
{
    if (!PT_REGS_PARM1(ctx))
        return 0;
    u32 pid = bpf_get_current_pid_tgid ();
    if (pid != PID)
        return 0;
    char str [80] = {};
    bpf_probe_read_user (&str, sizeof (str), (void *) PT_REGS_PARM1 (ctx));
    bpf_trace_printk ("%s\\n", &str);
    return 0;
}

"""
Summary of the above code:


1. The first line of the code is a comment, which is used to describe the purpose of this program.
2. The second line defines a macro that can be used in subsequent code, and its value is 0xffffffc0000839d0.
3. The third line defines a macro that can be used in subsequent code, and its value is 0xffffffc0009a7b40.
4. The fourth line defines a macro that can be used in subsequent code, and its value is 0xffffffc0009a7b60 (the address of the function


int do_entry (struct pt_regs *ctx)
{
    u32 pid;
    u64 ts;
    pid = bpf_get_current_pid_tgid ();
    ts = bpf_ktime_get_ns ();
    start.update (&pid, &ts);
    return 0;
}

"""
Summary of the above code:


1. We define a map called start, which is of type BPF_MAP_TYPE_HASH. This map will store the pid and timestamp when the process starts executing.
2. We define a function called do_entry, which will be invoked whenever a process starts executing (i.e., whenever it enters into kernel mode). In this function we get the current pid and timestamp using bpf helper functions and update our start map with these values.
3. We attach this program to kprobe __schedule using bpf() syscall from Python code (see below).

   


int do_return (struct pt_regs *ctx)
{
    u32 pid;
    u64 *tsp, delta;
    pid = bpf_get_current_pid_tgid ();
    tsp = start.lookup (&pid);
    if (tsp != 0) {
        delta = bpf_ktime_get_ns () - *tsp;
        dist.increment (bpf_log2l (delta / 1000));
        start.delete (&pid);
    }
    return 0;
}

"""
Summary of the above code:


1. The do_entry function is called when the probe hits the entry of a function. It gets the current PID and TGID, and stores it in a map along with the current time (in nanoseconds).
2. The do_return function is called when the probe hits the return of a function. It gets the current PID and TGID, looks up its start time in a map, calculates how long it took to execute that function, converts that duration to microseconds (by dividing by 1000), takes its logarithm base 2 (bpf_log2l), and increments an hist


int detect_ddos (struct pt_regs *ctx, void *skb)
{
    struct detectionPackets detectionPacket = {}
    ;
    u64 rcv_packets_nb_index = 0, rcv_packets_nb_inter = 1, *rcv_packets_nb_ptr;
    u64 rcv_packets_ts_index = 1, rcv_packets_ts_inter = 0, *rcv_packets_ts_ptr;
    rcv_packets_nb_ptr = rcv_packets.lookup (&rcv_packets_nb_index);
    rcv_packets_ts_ptr = rcv_packets.lookup (&rcv_packets_ts_index);
    if (rcv_packets_nb_ptr != 0 && rcv_packets_ts_ptr != 0) {
        rcv_packets_nb_inter = *rcv_packets_nb_ptr;
        rcv_packets_ts_inter = bpf_ktime_get_ns () - *rcv_packets_ts_ptr;
        if (rcv_packets_ts_inter < LEGAL_DIFF_TIMESTAMP_PACKETS) {
            rcv_packets_nb_inter++;
        }
        else {
            rcv_packets_nb_inter = 0;
        }
        if (rcv_packets_nb_inter > MAX_NB_PACKETS) {
            detectionPacket.nb_ddos_packets = rcv_packets_nb_inter;
            events.perf_submit (ctx, &detectionPacket, sizeof (detectionPacket));
        }
    }
    rcv_packets_ts_inter = bpf_ktime_get_ns ();
    rcv_packets.update (&rcv_packets_nb_index, &rcv_packets_nb_inter);
    rcv_packets.update (&rcv_packets_ts_index, &rcv_packets_ts_inter);
    return 0;
}

"""
Summary of the above code:


1. The first part of the code is the definition of a C struct that will be used to store information about detected DDoS attacks.
2. The second part of the code is the definition of a BPF map that will be used to store information about received packets (number and timestamp).
3. The third part is the definition of a BPF program, which has two functions:

    1. detect_ddos: This function checks if there are more than MAX_NB_PACKETS packets received in less than LEGAL_DIFF_TIMESTAMP_PACKETS nanosecond


static struct tcphdr *skb_to_tcphdr (const struct sk_buff *skb)
{
    return (struct tcphdr *) (skb->head + skb->transport_header);
}

"""
Summary of the above code:


1. The skb_to_tcphdr() function takes a pointer to an sk_buff structure and returns a pointer to the TCP header within that packet.
2. The tcp_hdrlen() function takes a pointer to the TCP header and returns the length of the TCP header in bytes.
3. The tcp_payload() function takes a pointer to an sk_buff structure and returns a pointer to the payload within that packet (i.e., after the TCP header).
4. The tcp_payloadlen() function takes pointers to both an sk_buff structure and its associated TCP


static inline struct iphdr *skb_to_iphdr (const struct sk_buff *skb)
{
    return (struct iphdr *) (skb->head + skb->network_header);
}

"""
Summary of the above code:


1. The skb_to_iphdr function is defined as an inline function.
2. The skb_to_iphdr function takes a pointer to the skb structure as its argument and returns a pointer to the iphdr structure.
3. The iphdr structure is located at the head of the packet plus network header offset in bytes from the beginning of the packet (skb->head + skb->network_header).





static inline struct ipv6hdr *skb_to_ip6hdr (const struct sk_buff *skb)
{
    return (struct ipv6hdr *) (skb->head + skb->network_header);
}

"""
Summary of the above code:


1. The skb_to_ip6hdr() function is defined as an inline function.
2. The skb_to_ip6hdr() function takes a pointer to the skb structure as its argument and returns a pointer to the ipv6hdr structure.
3. The skb->network_header field contains the offset of the network header from the start of the buffer, which is stored in skb->head . This offset is added to get a pointer to ipv6hdr .





int kprobe__nf_hook_slow (struct pt_regs *ctx, struct sk_buff *skb, struct nf_hook_state *state)
{
    struct start_data data = {}
    ;
    data.ts = bpf_ktime_get_ns ();
    data.hook = state->hook;
    data.pf = state->pf;
    COND u8 ip_proto;
    if (skb->protocol == htons (ETH_P_IP)) {
        struct iphdr *ip = skb_to_iphdr (skb);
        ip_proto = ip->protocol;
    }
    else if (skb->protocol == htons (ETH_P_IPV6)) {
        struct ipv6hdr *ip = skb_to_ip6hdr (skb);
        ip_proto = ip->nexthdr;
    }
    data.tcp_state = 0;
    if (ip_proto == 0x06) {
        struct tcphdr *tcp = skb_to_tcphdr (skb);
        u8 tcpflags = ((u_int8_t *) tcp)[13];
        if (((tcpflags & 1) + (tcpflags & 4)) > 0) {
            data.tcp_state = 3;
        }
        else if ((tcpflags & 0x02) > 0) {
            data.tcp_state = 1;
            if ((tcpflags & 16) > 0) {
                data.tcp_state = 2;
            }
        }
    }
    u32 idx = 0;
    sts.update (&idx, &data);
    return 0;
}

"""
Summary of the above code:


1. We define a struct start_data that will be used to store the data we want to collect.
2. We define a BPF_HASH called sts, which will be used to store the data collected by our kprobe__nf_hook_slow function. The key of this hash is an integer (u32) and the value is our struct start_data defined in step 1 above.
3. We define a kprobe__nf_hook_slow function that gets called every time nf hook slow is invoked in kernel space (see https://github.com/tor


int kretprobe__nf_hook_slow (struct pt_regs *ctx)
{
    u32 idx = 0;
    struct start_data *s;
    s = sts.lookup (&idx);
    if (!s || s->ts == 0) {
        return 0;
    }
    s->ts = bpf_ktime_get_ns () - s->ts;
    hist_key_t key = {}
    ;
    key.key.hook = s->hook;
    key.key.proto = s->pf;
    key.key.tcp_state = s->tcp_state;
    key.slot = bpf_log2l (s->ts / FACTOR);
    dist.increment (key);
    s->ts = 0;
    sts.update (&idx, s);
    return 0;
}

"""
Summary of the above code:


1. The first part of the code is to define the data structures used in BPF programs.
2. The second part of the code is to define a function that will be called when a packet enters nf_hook_slow, and this function will record the time when entering nf_hook_slow, and then store it in start_data structure.
3. The third part of the code is to define a function that will be called when a packet exits nf_hook_slow, and this function will calculate how long it takes for packets to pass through nf_hook slow, and then


int do_trace (struct pt_regs *ctx)
{
    uint64_t addr;
    char query [128];
    bpf_usdt_readarg (1, ctx, &addr);
    bpf_probe_read_user (&query, sizeof (query), (void *) addr);
    bpf_trace_printk ("%s\\n", query);
    return 0;
}

"""
Summary of the above code:


1. The first line imports the bcc library.
2. The second line defines a function called do_trace that takes a struct pt_regs as an argument and returns an int. This is the function that will be called when the USDT probe fires, and it will receive information about the context of the probe firing in its argument (the struct pt_regs).
3. The third line declares a variable addr of type uint64_t, which is used to store an address passed to this function by MySQL via USDT probes (more on this later).
4. The fourth line declares a


int hello (void *ctx)
{
    bpf_trace_printk ("Hello, World!\\n");
    return 0;
}

"""
Summary of the above code:


1. The first line of the code is a comment. Comments in C start with /* and end with */.
2. The next line includes the header file bpf_helpers.h, which contains various helper macros and functions that are needed to write eBPF programs in C (more on this later).
3. Next, we define a function called hello() that takes one argument of type void *ctx (more on this later) and returns an integer value 0 (which indicates success). This function prints “Hello, World!” using the bpf_trace_printk()


void trace_stack (struct pt_regs *ctx)
{
    u32 pid = bpf_get_current_pid_tgid () >> 32;
    FILTER struct data_t data = {}
    ;
    data.stack_id = stack_traces.get_stackid (ctx, 0), data.pid = pid;
    bpf_get_current_comm (&data.comm, sizeof (data.comm));
    events.perf_submit (ctx, &data, sizeof (data));
}

"""
Summary of the above code:


1. The first line of the code defines a BPF program that will be attached to the kprobe event sched_process_exit.
2. The second line of the code defines a BPF map named stack_traces, which is used to store stack traces for each process. This map has key type u64 and value type u64[10].
3. The third line of the code defines a BPF map named events, which is used to send data from kernel space to user space via perf ring buffer mechanism (see Chapter 4). This map has key type u32 and value type struct data


TRACEPOINT_PROBE (random, urandom_read)
{
    bpf_trace_printk ("%d\\n", args->got_bits);
    return 0;
}

"""
Summary of the above code:


1. The first line of the code is a comment.
2. The second line imports the tracepoints from the kernel header file, which is located at /sys/kernel/debug/tracing/events .
3. The third line defines a function called bpf_trace_printk , which prints out to stdout (the terminal). This function is defined in the C library that comes with BCC, and it takes one argument: a string to print out. In this case, we are printing out %d\\n , which means “print an integer followed by a newline


TRACEPOINT_PROBE (kvm, kvm_exit)
{
    u8 e = EXIT_REASON;
    u8 one = 1;
    if (args->exit_reason == EXIT_REASON) {
        bpf_trace_printk ("KVM_EXIT exit_reason : %d\\n", args->exit_reason);
        start.update (&e, &one);
    }
    return 0;
}

"""
Summary of the above code:


1. We are using the kvm_exit tracepoint to detect when a VM exits.
2. When the exit reason is EXIT_REASON, we increment a counter in start map.
3. The start map is used to count how many times we have seen this exit reason for each VMID (key).
4. If the counter reaches MAX_COUNT, then we print out an alert message and reset the counter back to 0 so that it can be reused for another VMID (key).




TRACEPOINT_PROBE (kvm, kvm_entry)
{
    u8 e = EXIT_REASON;
    u8 zero = 0;
    u8 *s = start.lookup (&e);
    if (s != NULL && *s == 1) {
        bpf_trace_printk ("KVM_ENTRY vcpu_id : %u\\n", args->vcpu_id);
        start.update (&e, &zero);
    }
    return 0;
}

"""
Summary of the above code:


1. The first TRACEPOINT_PROBE is for the kvm_entry event. It checks if the exit reason is EXIT_REASON and if it is, then it prints out the vcpu id and resets start[EXIT_REASON] to 0.
2. The second TRACEPOINT_PROBE is for the kvm_exit event. It checks if the exit reason is EXIT_REASON and if it isn't, then it sets start[EXIT_REASON] to 1 so that we can print out a message when we enter


TRACEPOINT_PROBE (kvm, kvm_hypercall)
{
    u8 e = EXIT_REASON;
    u8 zero = 0;
    u8 *s = start.lookup (&e);
    if (s != NULL && *s == 1) {
        bpf_trace_printk ("HYPERCALL nr : %d\\n", args->nr);
    }
    return 0;
}

"""
Summary of the above code:


1. The first part of the code is the same as in the previous example.
2. The second part defines a map called start, which will be used to store information about whether we want to trace a particular exit reason or not.
3. The third part defines a function called kvm_exit_reason, which is invoked when an exit reason occurs (i.e., when KVM exits). This function checks if we want to trace this particular exit reason and if so, it prints out some information about it and sets its value in start to 1 (meaning that we do want to trace this exit reason


int alloc_enter (struct pt_regs *ctx, size_t size)
{
    int key = stack_traces.get_stackid (ctx, BPF_F_USER_STACK);
    if (key < 0)
        return 0;
    u64 zero = 0, *val;
    val = calls.lookup_or_try_init (&key, &zero);
    if (val) {
        (*val) += size;
    }
    return 0;
}

"""
Summary of the above code:


1. We define a BPF program that will be attached to the kprobe for kmalloc.
2. The program defines a map called calls, which is of type hashmap with key as stackid and value as u64 (size_t).
3. The function alloc_enter() is called when the probe fires, it gets the stackid using get_stackid() helper function and increments the corresponding value in calls map by size parameter passed to kmalloc().
4. The function alloc_exit() is called when the probe returns, it does nothing but return 0 so that execution continues normally after


int kprobe__tcp_v4_connect (struct pt_regs *ctx, struct sock *sk)
{
    u32 pid = bpf_get_current_pid_tgid ();
    currsock.update (&pid, &sk);
    return 0;
}

"""
Summary of the above code:


1. We define a map called currsock, which is keyed by the PID of the process that is making the connection and has a value of type sock*. This will be used to store the socket pointer for each connection.
2. We define a kprobe__tcp_v4_connect function, which will be called whenever tcp_v4_connect() is invoked in kernel space. The first argument to this function is ctx, which contains information about registers at the time when tcp_v4_connect() was invoked (we don't need it here). The second argument sk contains a


int kretprobe__tcp_v4_connect (struct pt_regs *ctx)
{
    int ret = PT_REGS_RC (ctx);
    u32 pid = bpf_get_current_pid_tgid ();
    struct sock **skpp;
    skpp = currsock.lookup (&pid);
    if (skpp == 0) {
        return 0;
    }
    if (ret != 0) {
        currsock.delete (&pid);
        return 0;
    }
    struct sock *skp = *skpp;
    u32 saddr = skp->__sk_common.skc_rcv_saddr;
    u32 daddr = skp->__sk_common.skc_daddr;
    u16 dport = skp->__sk_common.skc_dport;
    bpf_trace_printk ("trace_tcp4connect %x %x %d\\n", saddr, daddr, ntohs (dport));
    currsock.delete (&pid);
    return 0;
}

"""
Summary of the above code:


1. We define a map to store the current socket pointer for each process.
2. In tcp_v4_connect, we save the socket pointer in the map and return immediately.
3. In kretprobe__tcp_v4_connect, we retrieve the saved socket pointer from the map and print out its source address, destination address and destination port number before deleting it from the map.



int printarg (struct urandom_read_args *args)
{
    bpf_trace_printk ("%d\\n", args->got_bits);
    return 0;
}

"""
Summary of the above code:


1. The first line of the code defines a map named urandom_read_count, which is a hash table with key type int and value type u64.
2. The second line of the code defines a function named do_count, which takes an argument args of type struct urandom_read_args and returns an integer value. This function increments the counter in the map for each call to getrandom() or read() on /dev/urandom by 1.
3. The third line of the code defines a function named printarg, which takes an argument args of type struct urandom_read


int count_sched (struct pt_regs *ctx, struct task_struct *prev)
{
    struct key_t key = {}
    ;
    u64 zero = 0, *val;
    key.curr_pid = bpf_get_current_pid_tgid ();
    key.prev_pid = prev->pid;
    val = stats.lookup_or_try_init (&key, &zero);
    if (val) {
        (*val)++;
    }
    return 0;
}

"""
Summary of the above code:


1. We define a BPF program that will be attached to the kprobe sched:sched_switch.
2. The program will be called every time a task is switched out of the CPU, and it will receive as arguments the context (ctx) and the previous task (prev).
3. We create a key_t structure that contains two fields: curr_pid and prev_pid, which are used to store the PIDs of both tasks involved in this context switch event.
4. We use stats as an array where we store all our data, using key as index for each entry


int do_trace (struct pt_regs *ctx)
{
    uint64_t addr;
    char path [128] = {0};
    bpf_usdt_readarg (6, ctx, &addr);
    bpf_probe_read_user (&path, sizeof (path), (void *) addr);
    bpf_trace_printk ("path:%s\\n", path);
    return 0;
}

"""
Summary of the above code:


1. The first line of the code is a comment, which is used to describe the purpose of this program.
2. The second line defines a macro that can be used in subsequent code, and its value is 0xffffffffff600000.
3. The third line defines a macro that can be used in subsequent code, and its value is 0xffffffffff600400.
4. The fourth line defines a macro that can be used in subsequent code, and its value is 0xffffffffff600800.
5 .The fifth row defines the function do_trace(), which takes one


int count (struct pt_regs *ctx)
{
    dist.increment (bpf_log2l (PT_REGS_RC (ctx)));
    return 0;
}

"""
Summary of the above code:


1. The BPF program is defined in the C language.
2. The BPF program has a single function called count, which takes a struct pt_regs *ctx as an argument and returns an int.
3. The count function increments the value of dist by 1, where dist is a histogram with 64 bins (0-63). Each bin represents one bit of the return code from the syscall being traced (PT_REGS_RC(ctx)).
4. The return value from count is 0, which means that this BPF program does not modify execution flow in any way;


int do_perf_event (struct bpf_perf_event_data *ctx)
{
    u32 pid = bpf_get_current_pid_tgid () >> 32;
    struct key_t key = {
        .pid = pid}
    ;
    bpf_get_current_comm (&key.name, sizeof (key.name));
    key.user_stack_id = stack_traces.get_stackid (&ctx->regs, BPF_F_USER_STACK);
    if (key.user_stack_id >= 0) {
        counts.increment (key);
    }
    return 0;
}

"""
Summary of the above code:


1. The do_perf_event function is called for every event that the kernel generates.
2. The bpf_get_current_pid_tgid() function returns the process ID and thread group ID of the current process, which we use to get just the process ID (the top 32 bits).
3. We then call bpf_get_current_comm() to get a string representation of the current command name, which we store in our key structure along with our PID value.
4. We then call stack traces’s get stackid method to retrieve a unique identifier


int do_sys_clone (void *ctx)
{
    struct {
        u64 ts;
        u64 magic;
    } data = {bpf_ktime_get_ns (), 0x12345678};

    int rc;
    if ((rc = events.perf_submit (ctx, &data, sizeof (data))) < 0)
        bpf_trace_printk ("perf_output failed: %d\\n", rc);
    int zero = 0;
    u64 *val = counters.lookup (&zero);
    if (val)
        lock_xadd (val, 1);
    return 0;
}

"""
Summary of the above code:


1. The program is attached to the kprobe for sys_clone.
2. It reads the current time and stores it in a struct along with a magic number (0x12345678).
3. It writes this data to a perf buffer using events.perf_submit().
4. It increments the value of counter 0 by 1 using counters[0].increment() or lock_xadd().

    Note: The above code will not compile because we have not defined events and counters yet, so let's do that now:

    


RAW_TRACEPOINT_PROBE (block_rq_complete)
{
    struct request *rq = (void *) ctx->args[0];
    unsigned int cmd_flags;
    u64 dur;
    size_t base, slot;
    if (!rq->io_start_time_ns)
        return 0;
    dur = bpf_ktime_get_ns () - rq->io_start_time_ns;
    slot = min_t (size_t, div_u64 (dur, 100 * NSEC_PER_MSEC), 99);
    lat_100ms.increment (slot);
    if (slot)
        return 0;
    slot = min_t (size_t, div_u64 (dur, NSEC_PER_MSEC), 99);
    lat_1ms.increment (slot);
    if (slot)
        return 0;
    slot = min_t (size_t, div_u64 (dur, 10 * NSEC_PER_USEC), 99);
    lat_10us.increment (slot);
    return 0;
}

"""
Summary of the above code:


1. The probe block_rq_insert is called when a request is inserted into the queue.
2. The probe block_rq_issue is called when a request is issued to the device.
3. The probe block_rq_complete is called when a request completes and returns to the queue layer for final processing (e.g., freeing).
4. We use bpf_ktime_get() to get the current time in nanoseconds, and subtract it from rq->io\start\time\ns, which was set in step 1 above, to get


int do_trace (struct pt_regs *ctx)
{
    u64 ts, *tsp, delta, key = 0;
    tsp = last.lookup (&key);
    if (tsp != NULL) {
        delta = bpf_ktime_get_ns () - *tsp;
        if (delta < 1000000000) {
            bpf_trace_printk ("%d\\n", delta / 1000000);
        }
        last.delete (&key);
    }
    ts = bpf_ktime_get_ns ();
    last.update (&key, &ts);
    return 0;
}

"""
Summary of the above code:


1. The do_trace function is called every time a syscall is made.
2. The last map stores the timestamp of the last syscall made by this process, and it's keyed by the pid of the process making the syscall.
3. If there was a previous timestamp stored in last, then we calculate how much time has passed since that previous call and print it out to stdout (in milliseconds).
4. We update our map with a new timestamp for this process' pid so that we can use it next time around to calculate how long it took between calls for this


int trace_unix_stream_read_actor (struct pt_regs *ctx)
{
    u32 zero = 0;
    int ret = PT_REGS_RC (ctx);
    u64 pid_tgid = bpf_get_current_pid_tgid ();
    u32 pid = pid_tgid >> 32;
    u32 tid = pid_tgid;
    FILTER_PID struct sk_buff *skb = (struct sk_buff *) PT_REGS_PARM1 (ctx);
    struct recv_data_t *data = unix_data.lookup (&zero);
    if (!data)
        return 0;
    unsigned int data_len = skb->len;
    if (data_len > MAX_PKT)
        return 0;
    void *iodata = (void *) skb->data;
    data->recv_len = data_len;
    bpf_probe_read (data->pkt, data_len, iodata);
    unix_recv_events.perf_submit (ctx, data, data_len + sizeof (u32));
    return 0;
}

"""
Summary of the above code:


1. The first part of the code is the same as before, where we define a map to store data and a perf event to send it.
2. We then define two functions: trace_unix_stream_write_actor and trace_unix_stream_read_actor, which are called when data is written or read from a unix socket respectively. These functions are called with the following parameters:
    1. struct pt\*regs \*ctx - This contains information about the current process context (like registers)
    2. struct sk\*buff \*skb -


int hello (struct pt_regs *ctx)
{
    struct data_t data = {}
    ;
    struct bpf_pidns_info ns = {}
    ;
    if (bpf_get_ns_current_pid_tgid (DEV, INO, &ns, sizeof (struct bpf_pidns_info)))
        return 0;
    data.pid = ns.pid;
    data.ts = bpf_ktime_get_ns ();
    bpf_get_current_comm (&data.comm, sizeof (data.comm));
    events.perf_submit (ctx, &data, sizeof (data));
    return 0;
}

"""
Summary of the above code:


1. The program is a tracepoint program, which means it will be attached to a tracepoint.
2. The tracepoint is defined by the macro TRACEPOINT_PROBE(vfs, vfs_getattr). This macro expands to the following code:

    BPF_TRACEPOINT_PROBE(vfs, vfs_getattr) {
        struct data_t data = {}
        ;
        struct bpf_pidns_info ns = {}
        ;

    }

    BPF


int hello (struct pt_regs *ctx)
{
    struct data_t data = {}
    ;
    data.pid = bpf_get_current_pid_tgid ();
    data.ts = bpf_ktime_get_ns ();
    bpf_get_current_comm (&data.comm, sizeof (data.comm));
    events.perf_submit (ctx, &data, sizeof (data));
    return 0;
}

"""
Summary of the above code:


1. The first line of the code is a comment, which is ignored by the compiler.
2. The second line imports the header file bpf_helpers.h, which contains some helper functions and macros that are used in this program to make it easier to write BPF programs in C language.
3. The third line defines a structure data_t that will be used to store data for each event we want to trace (in this case, we want to trace when a process starts). This structure has three fields: pid (process ID), ts (timestamp) and comm (command name).

