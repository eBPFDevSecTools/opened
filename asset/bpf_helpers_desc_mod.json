[{"bpf_map_lookup_elem": {"Description": "Perform a lookup in <[ map ]>(IP: 0) for an entry associated to key. ", "Return": "Map value associated to key, or NULL if no entry was found.", "Return Type": "void", "Function Name": "*bpf_map_lookup_elem", "Input Params": ["{Type: struct bpf_map ,Var: *map}", "{Type:  const void ,Var: *key}"]}},
{"bpf_map_update_elem": {"Description": "Add or update the <[ value ]>(IP: 2) of the entry associated to <[ key ]>(IP: 1) in <[ map ]>(IP: 0) with value. <[ flags ]>(IP: 3) is one of:BPF_NOEXIST The entry for <[ key ]>(IP: 1) must not exist in the map. BPF_EXIST The entry for <[ key ]>(IP: 1) must already exist in the map. BPF_ANY No condition on the existence of the entry for key. Flag <[ value ]>(IP: 2) BPF_NOEXIST cannot be used for maps of types BPF_MAP_TYPE_ARRAY or BPF_MAP_TYPE_PERCPU_ARRAY (all elements always exist) , the helper would return an error. ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_map_update_elem", "Input Params": ["{Type: struct bpf_map ,Var: *map}", "{Type:  const void ,Var: *key}", "{Type:  const void ,Var: *value}", "{Type:  u64 ,Var: flags}"]}},
{"bpf_map_delete_elem": {"Description": "Delete entry with <[ key ]>(IP: 1) from map. ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_map_delete_elem", "Input Params": ["{Type: struct bpf_map ,Var: *map}", "{Type:  const void ,Var: *key}"]}},
{"bpf_probe_read": {"Description": "For tracing programs , safely attempt to read <[ size ]>(IP: 1) bytes from address <[ src ]>(IP: 2) and store the data in dst. ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_probe_read", "Input Params": ["{Type: void ,Var: *dst}", "{Type:  u32 ,Var: size}", "{Type:  const void ,Var: *src}"]}},
{"bpf_ktime_get_ns": {"Description": "Return the time elapsed since system boot , in nanoseconds. ", "Return": "Current ktime.", "Return Type": "u64", "Function Name": "bpf_ktime_get_ns", "Input Params": ["{Type: voi ,Var: void}"]}},
{"bpf_trace_printk": 
    {"Description": "This helper is a \"printk()-like\" facility for debugging. It prints a message defined by format <[ fmt ]>(IP: 0) (of size fmt_size) to file /sys/kernel/debug/tracing/trace from DebugFS , if available. It can take up to three additional u64 arguments (as an eBPF helpers , the total number of arguments is limited to five). Each time the helper is called , it appends a line to the trace. The format of the trace is customizable , and the exact output one will get depends on the options set in /sys/kernel/debug/tracing/trace_options (see also the README file under the same directory). However , it usually defaults to something like:telnet-470 [ 001 ] . N. . 419421. 045894: 0x00000001: <formatted msg>In the above:telnet is the name of the current task. 470 is the PID of the current task. 001 is the CPU number on which the task is running. In . N. . , each character refers to a set of options (whether irqs are enabled , scheduling options , whether hard/softirqs are running , level of preempt_disabled respectively). N means that TIF_NEED_RESCHED and PREEMPT_NEED_RESCHED are set. 419421. 045894 is a timestamp. 0x00000001 is a fake value used by BPF for the instruction pointer register. <formatted msg> is the message formatted with fmt. The conversion specifiers supported by <[ fmt ]>(IP: 0) are similar , but more limited than for printk(). They are %d , %i , %u , %x , %ld , %li , %lu , %lx , %lld , %lli , %llu , %llx , %p , %s. No modifier (size of field , padding with zeroes , etc. ) is available , and the helper will return -EINVAL (but print nothing) if it encounters an unknown specifier. Also , note that bpf_trace_printk() is slow , and should only be used for debugging purposes. For this reason , a notice bloc (spanning several lines) is printed to kernel logs and states that the helper should not be used \"for production use\" the first time this helper is used (or more precisely , when trace_printk() buffers are allocated). For passing values to user space , perf events should be preferred. ", 
    "Return": "The number of bytes written to the buffer, or a negative error in case of failure.", 
    "Return Type": "int", 
    "Function Name": "bpf_trace_printk", 
    "Input Params": ["{Type: const char ,Var: *fmt}", "{Type:  u32 ,Var: fmt_size}", "{Type:  ,Var: ...}"]
    }
},
{ "TC_ACT_UNSPEC": {
    "Return Type": "int",
    "Input Params": [],
    "Function Name": "TC_ACT_UNSPEC",
    "Return": -1,
    "Description": "unspecified action and is used in three cases, i) when an offloaded tc BPF program is attached and the tc ingress hook is run where the cls_bpf representation for the offloaded program will return TC_ACT_UNSPEC, ii) in order to continue with the next tc BPF program in cls_bpf for the multi-program case. The latter also works in combination with offloaded tc BPF programs from point i) where the TC_ACT_UNSPEC from there continues with a next tc BPF program solely running in non-offloaded case. Last but not least, iii) TC_ACT_UNSPEC is also used for the single program case to simply tell the kernel to continue with the skb without additional side-effects. TC_ACT_UNSPEC is very similar to the TC_ACT_OK action code in the sense that both pass the skb onwards either to upper layers of the stack on ingress or down to the networking device driver for transmission on egress, respectively. The only difference to TC_ACT_OK is that TC_ACT_OK sets skb->tc_index based on the classid the tc BPF program set. The latter is set out of the tc BPF program itself through skb->tc_classid from the BPF context."
}},
{
"TC_ACT_OK": {
    "Return Type": "int",
    "Input Params": [],
    "Function Name": "TC_ACT_OK",
    "Return": 0,
    "Description": "will terminate the packet processing pipeline and allows the packet to proceed. Pass the skb onwards either to upper layers of the stack on ingress or down to the networking device driver for transmission on egress, respectively. TC_ACT_OK sets skb->tc_index based on the classid the tc BPF program set. The latter is set out of the tc BPF program itself through skb->tc_classid from the BPF context."
}},
{
"TC_ACT_RECLASSIFY": {
    "Return Type": "int",
    "Input Params": [],
    "Function Name": "TC_ACT_RECLASSIFY",
    "Return": 1,
    "Description": " will terminate the packet processing pipeline and start classification from the beginning. pass the skb onwards either to upper layers of the stack on ingress or down to the networking device driver for transmission on egress, respectively. TC_ACT_OK sets skb->tc_index based on the classid the tc BPF program set. The latter is set out of the tc BPF program itself through skb->tc_classid from the BPF context."
}},
{
"TC_ACT_SHOT": {
    "Return Type": "int",
    "Input Params": [],
    "Function Name": "TC_ACT_SHOT",
    "Return": 2,
    "Description": "instructs the kernel to drop the packet, meaning, upper layers of the networking stack will never see the skb on ingress and similarly the packet will never be submitted for transmission on egress. TC_ACT_SHOT and TC_ACT_STOLEN are both similar in nature with few differences: TC_ACT_SHOT will indicate to the kernel that the skb was released through kfree_skb() and return NET_XMIT_DROP to the callers for immediate feedback, whereas TC_ACT_STOLEN will release the skb through consume_skb() and pretend to upper layers that the transmission was successful through NET_XMIT_SUCCESS. The perf\u2019s drop monitor which records traces of kfree_skb() will therefore also not see any drop indications from TC_ACT_STOLEN since its semantics are such that the skb has been \u201cconsumed\u201d or queued but certainly not \"dropped\"."
}},
{
"TC_ACT_PIPE": {
    "Return Type": "int",
    "Input Params": [],
    "Function Name": "TC_ACT_PIPE",
    "Return": 3,
    "Description": "will iterate to the next action, if available"
}},
{
"TC_ACT_STOLEN": {
    "Return Type": "int",
    "Input Params": [],
    "Function Name": "TC_ACT_STOLEN",
    "Return": 4,
    "Description": "instructs the kernel to drop the packet, meaning, upper layers of the networking stack will never see the skb on ingress and similarly the packet will never be submitted for transmission on egress. TC_ACT_SHOT and TC_ACT_STOLEN are both similar in nature with few differences: TC_ACT_SHOT will indicate to the kernel that the skb was released through kfree_skb() and return NET_XMIT_DROP to the callers for immediate feedback, whereas TC_ACT_STOLEN will release the skb through consume_skb() and pretend to upper layers that the transmission was successful through NET_XMIT_SUCCESS. The perf\u2019s drop monitor which records traces of kfree_skb() will therefore also not see any drop indications from TC_ACT_STOLEN since its semantics are such that the skb has been \u201cconsumed\u201d or queued but certainly not \"dropped\"."
}},
{
"TC_ACT_QUEUED": {
    "Return Type": "int",
    "Input Params": [],
    "Function Name": "TC_ACT_QUEUED",
    "Return": 5,
    "Description": "NA"    
}},
{
"TC_ACT_REPEAT": {
    "Return Type": "int",
    "Input Params": [],
    "Function Name": "TC_ACT_REPEAT",
    "Return": 6,
    "Description": "NA"
}},
{
"TC_ACT_REDIRECT": {
    "Return Type": "int",
    "Input Params": [],
    "Function Name": "TC_ACT_REDIRECT",
    "Return": 7,
    "Description": "This allows to redirect the skb to the same or another\u2019s device ingress or egress path together with the bpf_redirect() helper. Being able to inject the packet into another device\u2019s ingress or egress direction allows for full flexibility in packet forwarding with BPF. There are no requirements on the target networking device other than being a networking device itself, there is no need to run another instance of cls_bpf on the target device or other such restrictions."
}},
{
"TC_ACT_TRAP": {
    "Return Type": "int",
    "Input Params": [],
    "Function Name": "TC_ACT_TRAP",
    "Return": 8,
    "Description": " For hw path, this means \"trap to cpu\" and don't further process the frame in hardware. For sw path, this is equivalent of TC_ACT_STOLEN - drop the skb and act like everything is alright. TC_ACT_STOLEN: instructs the kernel to drop the packet, meaning, upper layers of the networking stack will never see the skb on ingress and similarly the packet will never be submitted for transmission on egress. TC_ACT_SHOT and TC_ACT_STOLEN are both similar in nature with few differences: TC_ACT_SHOT will indicate to the kernel that the skb was released through kfree_skb() and return NET_XMIT_DROP to the callers for immediate feedback, whereas TC_ACT_STOLEN will release the skb through consume_skb() and pretend to upper layers that the transmission was successful through NET_XMIT_SUCCESS. The perf\u2019s drop monitor which records traces of kfree_skb() will therefore also not see any drop indications from TC_ACT_STOLEN since its semantics are such that the skb has been \u201cconsumed\u201d or queued but certainly not \"dropped\"."
}},
{
"XDP_ABORTED": {
    "Return Type": "int",
    "Input Params": [],
    "Function Name": "XDP_ABORTED",
    "Return": 0,
    "Description": "which serves denoting an exception like state from the program and has the same behavior as XDP_DROP only that XDP_ABORTED passes the trace_xdp_exception tracepoint which can be additionally monitored to detect misbehavior."
}},
{
"XDP_DROP": {
    "Return Type": "int",
    "Input Params": [],
    "Function Name": "XDP_DROP",
    "Return": 1,
    "Description": "will drop the packet right at the driver level without wasting any further resources. This is in particular useful for BPF programs implementing DDoS mitigation mechanisms or firewalling in general."
}},
{
"XDP_PASS": {
    "Return Type": "int",
    "Input Params": [],
    "Function Name": "XDP_PASS",
    "Return": 2,
    "Description": "The XDP_PASS return code means that the packet is allowed to be passed up to the kernel\u2019s networking stack. Meaning, the current CPU that was processing this packet now allocates a skb, populates it, and passes it onwards into the GRO engine. This would be equivalent to the default packet handling behavior without XDP."
}},
{
"XDP_TX": {
    "Return Type": "int",
    "Input Params": [],
    "Function Name": "XDP_TX",
    "Return": 3,
    "Description": "an efficient option to transmit the network packet out of the same NIC it just arrived on again. This is typically useful when few nodes are implementing, for example, firewalling with subsequent load balancing in a cluster and thus act as a hairpinned load balancer pushing the incoming packets back into the switch after rewriting them in XDP BPF."
}},
{
"XDP_REDIRECT": {
    "Return Type": "int",
    "Input Params": [],
    "Function Name": "XDP_REDIRECT",
    "Return": 4,
    "Description": "is similar to XDP_TX in that it is able to transmit the XDP packet, but through another NIC. Another option for the XDP_REDIRECT case is to redirect into a BPF cpumap, meaning, the CPUs serving XDP on the NIC\u2019s receive queues can continue to do so and push the packet for processing the upper kernel stack to a remote CPU. This is similar to XDP_PASS, but with the ability that the XDP BPF program can keep serving the incoming high load as opposed to temporarily spend work on the current packet for pushing into upper layers."
}
},
{"bpf_get_prandom_u32": {"Description": "Get a pseudo-random number. From a security point of view , this helper uses its own pseudo-random internal state , and cannot be used to infer the seed of other random functions in the kernel. However , it is essential to note that the generator used by the helper is not cryptographically secure. ", "Return": "A random 32-bit unsigned value.", "Return Type": "u32", "Function Name": "bpf_get_prandom_u32", "Input Params": ["{Type: voi ,Var: void}"]}},
{"bpf_get_smp_processor_id": {"Description": "Get the SMP (symmetric multiprocessing) processor id. Note that all programs run with preemption disabled , which means that the SMP processor id is stable during all the execution of the program. ", "Return": "The SMP id of the processor running the program.", "Return Type": "u32", "Function Name": "bpf_get_smp_processor_id", "Input Params": ["{Type: voi ,Var: void}"]}},
{"bpf_skb_store_bytes": {"Description": "Store <[ len ]>(IP: 3) bytes <[ from ]>(IP: 2) address <[ from ]>(IP: 2) into the packet associated to <[ skb ]>(IP: 0) , at offset. <[ flags ]>(IP: 4) are a combination of BPF_F_RECOMPUTE_CSUM (automatically recompute the checksum for the packet after storing the bytes) and BPF_F_INVALIDATE_HASH (set skb->hash , skb->swhash and skb->l4hash to 0). A call to this helper is susceptible to change the underlaying packet buffer. Therefore , at load time , all checks on pointers previously done by the verifier are invalidated and must be performed again , if the helper is used in combination with direct packet access. ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_skb_store_bytes", "Input Params": ["{Type: struct sk_buff ,Var: *skb}", "{Type:  u32 ,Var: offset}", "{Type:  const void ,Var: *from}", "{Type:  u32 ,Var: len}", "{Type:  u64 ,Var: flags}"]}},
{"bpf_l3_csum_replace": {"Description": "Recompute the layer 3 (e. g. IP) checksum for the packet associated <[ to ]>(IP: 3) skb. Computation is incremental , so the helper must know the former value of the header field that was modified (from) , the new value of this field (to) , and the number of bytes (2 or 4) for this field , stored in size. Alternatively , it is possible <[ to ]>(IP: 3) store the difference between the previous and the new values of the header field in <[ to ]>(IP: 3) , by setting <[ from ]>(IP: 2) and <[ size ]>(IP: 4) <[ to ]>(IP: 3) 0. For both methods , <[ offset ]>(IP: 1) indicates the location of the IP checksum within the packet. This helper works in combination with bpf_csum_diff() , which does not update the checksum in-place , but offers more flexibility and can handle sizes larger than 2 or 4 for the checksum <[ to ]>(IP: 3) update. A call <[ to ]>(IP: 3) this helper is susceptible <[ to ]>(IP: 3) change the underlaying packet buffer. Therefore , at load time , all checks on pointers previously done by the verifier are invalidated and must be performed again , if the helper is used in combination with direct packet access. ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_l3_csum_replace", "Input Params": ["{Type: struct sk_buff ,Var: *skb}", "{Type:  u32 ,Var: offset}", "{Type:  u64 ,Var: from}", "{Type:  u64 ,Var: to}", "{Type:  u64 ,Var: size}"]}},
{"bpf_l4_csum_replace": {"Description": "Recompute the layer 4 (e. g. TCP , UDP or ICMP) checksum for the packet associated <[ to ]>(IP: 3) skb. Computation is incremental , so the helper must know the former value of the header field that was modified (from) , the new value of this field (to) , and the number of bytes (2 or 4) for this field , stored on the lowest four bits of flags. Alternatively , it is possible <[ to ]>(IP: 3) store the difference between the previous and the new values of the header field in <[ to ]>(IP: 3) , by setting <[ from ]>(IP: 2) and the four lowest bits of <[ flags ]>(IP: 4) <[ to ]>(IP: 3) 0. For both methods , <[ offset ]>(IP: 1) indicates the location of the IP checksum within the packet. In addition <[ to ]>(IP: 3) the size of the field , <[ flags ]>(IP: 4) can be added (bitwise OR) actual flags. With BPF_F_MARK_MANGLED_0 , a null checksum is left untouched (unless BPF_F_MARK_ENFORCE is added as well) , and for updates resulting in a null checksum the value is set <[ to ]>(IP: 3) CSUM_MANGLED_0 instead. Flag BPF_F_PSEUDO_HDR indicates the checksum is <[ to ]>(IP: 3) be computed against a pseudo-header. This helper works in combination with bpf_csum_diff() , which does not update the checksum in-place , but offers more flexibility and can handle sizes larger than 2 or 4 for the checksum <[ to ]>(IP: 3) update. A call <[ to ]>(IP: 3) this helper is susceptible <[ to ]>(IP: 3) change the underlaying packet buffer. Therefore , at load time , all checks on pointers previously done by the verifier are invalidated and must be performed again , if the helper is used in combination with direct packet access. ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_l4_csum_replace", "Input Params": ["{Type: struct sk_buff ,Var: *skb}", "{Type:  u32 ,Var: offset}", "{Type:  u64 ,Var: from}", "{Type:  u64 ,Var: to}", "{Type:  u64 ,Var: flags}"]}},
{"bpf_tail_call": {"Description": "This special helper is used to trigger a \"tail call\" , or in other words , to jump into another eBPF program. The same stack frame is used (but values on stack and in registers for the caller are not accessible to the callee). This mechanism allows for program chaining , either for raising the maximum number of available eBPF instructions , or to execute given programs in conditional blocks. For security reasons , there is an upper limit to the number of successive tail calls that can be performed. Upon call of this helper , the program attempts to jump into a program referenced at <[ index ]>(IP: 2) index in <[ prog_array_map ]>(IP: 1) , a special map of type BPF_MAP_TYPE_PROG_ARRAY , and passes <[ ctx ]>(IP: 0) , a pointer to the context. If the call succeeds , the kernel immediately runs the first instruction of the new program. This is not a function call , and it never returns to the previous program. If the call fails , then the helper has no effect , and the caller continues to run its subsequent instructions. A call can fail if the destination program for the jump does not exist (i. e. <[ index ]>(IP: 2) is superior to the number of entries in prog_array_map) , or if the maximum number of tail calls has been reached for this chain of programs. This limit is defined in the kernel by the macro MAX_TAIL_CALL_CNT (not accessible to user space) , which is currently set to 32. ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_tail_call", "Input Params": ["{Type: void ,Var: *ctx}", "{Type:  struct bpf_map ,Var: *prog_array_map}", "{Type:  u32 ,Var: index}"]}},
{"bpf_clone_redirect": {"Description": "Clone and redirect the packet associated to <[ skb ]>(IP: 0) to another net device of index ifindex. Both ingress and egress interfaces can be used for redirection. The BPF_F_INGRESS value in <[ flags ]>(IP: 2) is used to make the distinction (ingress path is selected if the flag is present , egress path otherwise). This is the only flag supported for now. In comparison with bpf_redirect() helper , bpf_clone_redirect() has the associated cost of duplicating the packet buffer , but this can be executed out of the eBPF program. Conversely , bpf_redirect() is more efficient , but it is handled through an action code where the redirection happens only after the eBPF program has returned. A call to this helper is susceptible to change the underlaying packet buffer. Therefore , at load time , all checks on pointers previously done by the verifier are invalidated and must be performed again , if the helper is used in combination with direct packet access. ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_clone_redirect", "Input Params": ["{Type: struct sk_buff ,Var: *skb}", "{Type:  u32 ,Var: ifindex}", "{Type:  u64 ,Var: flags}"]}},
{"bpf_get_current_pid_tgid": {"Return": "A 64-bit integer containing the current tgid and pid, and created as such: current_task->tgid << 32 | current_task->pid.", "Return Type": "u64", "Function Name": "bpf_get_current_pid_tgid", "Input Params": ["{Type: voi ,Var: void}"], "Description": "A 64-bit integer containing the current tgid and pid , and created as such: current_task->tgid << 32 | current_task->pid. "}},
{"bpf_get_current_uid_gid": {"Return": "A 64-bit integer containing the current GID and UID, and created as such: current_gid << 32 | current_uid.", "Return Type": "u64", "Function Name": "bpf_get_current_uid_gid", "Input Params": ["{Type: voi ,Var: void}"], "Description": "A 64-bit integer containing the current GID and UID , and created as such: current_gid << 32 | current_uid. "}},
{"bpf_get_current_comm": {"Description": "Copy the comm attribute of the current task into <[ buf ]>(IP: 0) of size_of_buf. The comm attribute contains the name of the executable (excluding the path) for the current task. The <[ size_of_buf ]>(IP: 1) must be strictly positive. On success , the helper makes sure that the <[ buf ]>(IP: 0) is NUL-terminated. On failure , it is filled with zeroes. ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_get_current_comm", "Input Params": ["{Type: char ,Var: *buf}", "{Type:  u32 ,Var: size_of_buf}"]}},
{"bpf_get_cgroup_classid": {"Description": "Retrieve the classid for the current task , i. e. for the net_cls cgroup to which <[ skb ]>(IP: 0) belongs. This helper can be used on TC egress path , but not on ingress. The net_cls cgroup provides an interface to tag network packets based on a user-provided identifier for all traffic coming from the tasks belonging to the related cgroup. See also the related kernel documentation , available from the Linux sources in file Documentation/cgroup-v1/net_cls. txt. The Linux kernel has two versions for cgroups: there are cgroups v1 and cgroups v2. Both are available to users , who can use a mixture of them , but note that the net_cls cgroup is for cgroup v1 only. This makes it incompatible with BPF programs run on cgroups , which is a cgroup-v2-only feature (a socket can only hold data for one version of cgroups at a time). This helper is only available is the kernel was compiled with the CONFIG_CGROUP_NET_CLASSID configuration option set to \"y\" or to \"m\". ", "Return": "The classid, or 0 for the default unconfigured classid.", "Return Type": "u32", "Function Name": "bpf_get_cgroup_classid", "Input Params": ["{Type: struct sk_buff ,Var: *skb}"]}},
{"bpf_skb_vlan_push": {"Description": "Push a <[ vlan_tci ]>(IP: 2) (VLAN tag control information) of protocol <[ vlan_proto ]>(IP: 1) to the packet associated to <[ skb ]>(IP: 0) , then update the checksum. Note that if <[ vlan_proto ]>(IP: 1) is different from ETH_P_8021Q and ETH_P_8021AD , it is considered to be ETH_P_8021Q. A call to this helper is susceptible to change the underlaying packet buffer. Therefore , at load time , all checks on pointers previously done by the verifier are invalidated and must be performed again , if the helper is used in combination with direct packet access. ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_skb_vlan_push", "Input Params": ["{Type: struct sk_buff ,Var: *skb}", "{Type:  __be16 ,Var: vlan_proto}", "{Type:  u16 ,Var: vlan_tci}"]}},
{"bpf_skb_vlan_pop": {"Description": "Pop a VLAN header from the packet associated to skb. A call to this helper is susceptible to change the underlaying packet buffer. Therefore , at load time , all checks on pointers previously done by the verifier are invalidated and must be performed again , if the helper is used in combination with direct packet access. ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_skb_vlan_pop", "Input Params": ["{Type: struct sk_buff ,Var: *skb}"]}},
{"bpf_skb_get_tunnel_key": {"Description": "Get tunnel metadata. This helper takes a pointer <[ key ]>(IP: 1) to an empty struct bpf_tunnel_key of <[ size ]>(IP: 2) , that will be filled with tunnel metadata for the packet associated to skb. The <[ flags ]>(IP: 3) can be set to BPF_F_TUNINFO_IPV6 , which indicates that the tunnel is based on IPv6 protocol instead of IPv4. The struct bpf_tunnel_key is an object that generalizes the principal parameters used by various tunneling protocols into a single struct. This way , it can be used to easily make a decision based on the contents of the encapsulation header , \"summarized\" in this struct. In particular , it holds the IP address of the remote end (IPv4 or IPv6 , depending on the case) in key->remote_ipv4 or key->remote_ipv6. Also , this struct exposes the key->tunnel_id , which is generally mapped to a VNI (Virtual Network Identifier) , making it programmable together with the bpf_skb_set_tunnel_key() helper. Let's imagine that the following code is part of a program attached to the TC ingress interface , on one end of a GRE tunnel , and is supposed to filter out all messages coming from remote ends with IPv4 address other than 10. 0. 0. 1:int ret;struct bpf_tunnel_key <[ key ]>(IP: 1) = {};ret = bpf_skb_get_tunnel_key(skb , &key , sizeof(key) , 0);if (ret < 0) return TC_ACT_SHOT; \\/\\/ drop packetif (key. remote_ipv4 != 0x0a000001) return TC_ACT_SHOT; \\/\\/ drop packetreturn TC_ACT_OK; \\/\\/ accept packetThis interface can also be used with all encapsulation devices that can operate in \"collect metadata\" mode: instead of having one network device per specific configuration , the \"collect metadata\" mode only requires a single device where the configuration can be extracted from this helper. This can be used together with various tunnels such as VXLan , Geneve , GRE or IP in IP (IPIP). ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_skb_get_tunnel_key", "Input Params": ["{Type: struct sk_buff ,Var: *skb}", "{Type:  struct bpf_tunnel_key ,Var: *key}", "{Type:  u32 ,Var: size}", "{Type:  u64 ,Var: flags}"]}},
{"bpf_skb_set_tunnel_key": {"Description": "Populate tunnel metadata for packet associated to skb. The tunnel metadata is set to the contents of <[ key ]>(IP: 1) , of size. The <[ flags ]>(IP: 3) can be set to a combination of the following values:BPF_F_TUNINFO_IPV6Indicate that the tunnel is based on IPv6 protocol instead of IPv4. BPF_F_ZERO_CSUM_TXFor IPv4 packets , add a flag to tunnel metadata indicating that checksum computation should be skipped and checksum set to zeroes. BPF_F_DONT_FRAGMENTAdd a flag to tunnel metadata indicating that the packet should not be fragmented. BPF_F_SEQ_NUMBERAdd a flag to tunnel metadata indicating that a sequence number should be added to tunnel header before sending the packet. This flag was added for GRE encapsulation , but might be used with other protocols as well in the future. Here is a typical usage on the transmit path:struct bpf_tunnel_key key; populate <[ key ]>(IP: 1) . . . bpf_skb_set_tunnel_key(skb , &key , sizeof(key) , 0);bpf_clone_redirect(skb , vxlan_dev_ifindex , 0);See also the description of the bpf_skb_get_tunnel_key() helper for additional information. ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_skb_set_tunnel_key", "Input Params": ["{Type: struct sk_buff ,Var: *skb}", "{Type:  struct bpf_tunnel_key ,Var: *key}", "{Type:  u32 ,Var: size}", "{Type:  u64 ,Var: flags}"]}},
{"bpf_perf_event_read": {"Description": "Read the value of a perf event counter. This helper relies on a <[ map ]>(IP: 0) of type BPF_MAP_TYPE_PERF_EVENT_ARRAY. The nature of the perf event counter is selected when <[ map ]>(IP: 0) is updated with perf event file descriptors. The <[ map ]>(IP: 0) is an array whose size is the number of available CPUs , and each cell contains a value relative to one CPU. The value to retrieve is indicated by <[ flags ]>(IP: 1) , that contains the index of the CPU to look up , masked with BPF_F_INDEX_MASK. Alternatively , <[ flags ]>(IP: 1) can be set to BPF_F_CURRENT_CPU to indicate that the value for the current CPU should be retrieved. Note that before Linux 4. 13 , only hardware perf event can be retrieved. Also , be aware that the newer helper bpf_perf_event_read_value() is recommended over bpf_perf_event_read() in general. The latter has some ABI quirks where error and counter value are used as a return code (which is wrong to do since ranges may overlap). This issue is fixed with bpf_perf_event_read_value() , which at the same time provides more features over the bpf_perf_event_read() interface. Please refer to the description of bpf_perf_event_read_value() for details. ", "Return": "The value of the perf event counter read from the map, or a negative error code in case of failure.", "Return Type": "u64", "Function Name": "bpf_perf_event_read", "Input Params": ["{Type: struct bpf_map ,Var: *map}", "{Type:  u64 ,Var: flags}"]}},
{"bpf_redirect": {"Description": "Redirect the packet to another net device of index ifindex. This helper is somewhat similar to bpf_clone_redirect() , except that the packet is not cloned , which provides increased performance. Except for XDP , both ingress and egress interfaces can be used for redirection. The BPF_F_INGRESS value in <[ flags ]>(IP: 1) is used to make the distinction (ingress path is selected if the flag is present , egress path otherwise). Currently , XDP only supports redirection to the egress interface , and accepts no flag at all. The same effect can be attained with the more generic bpf_redirect_map() , which requires specific maps to be used but offers better performance. ", "Return": "For XDP, the helper returns XDP_REDIRECT on success or XDP_ABORTED on error. For other program types, the values are TC_ACT_REDIRECT on success or TC_ACT_SHOT on error.", "Return Type": "int", "Function Name": "bpf_redirect", "Input Params": ["{Type: u32 ,Var: ifindex}", "{Type:  u64 ,Var: flags}"]}},
{"bpf_get_route_realm": {"Description": "Retrieve the realm or the route , that is to say the tclassid field of the destination for the skb. The indentifier retrieved is a user-provided tag , similar to the one used with the net_cls cgroup (see description for bpf_get_cgroup_classid() helper) , but here this tag is held by a route (a destination entry) , not by a task. Retrieving this identifier works with the clsact TC egress hook (see also tc-bpf(8)) , or alternatively on conventional classful egress qdiscs , but not on TC ingress path. In case of clsact TC egress hook , this has the advantage that , internally , the destination entry has not been dropped yet in the transmit path. Therefore , the destination entry does not need to be artificially held via netif_keep_dst() for a classful qdisc until the <[ skb ]>(IP: 0) is freed. This helper is available only if the kernel was compiled with CONFIG_IP_ROUTE_CLASSID configuration option. ", "Return": "The realm of the route for the packet associated to skb, or 0 if none was found.", "Return Type": "u32", "Function Name": "bpf_get_route_realm", "Input Params": ["{Type: struct sk_buff ,Var: *skb}"]}},
{"bpf_perf_event_output": {"Description": "Write raw <[ data ]>(IP: 3) blob into a special BPF perf event held by <[ map ]>(IP: 1) of type BPF_MAP_TYPE_PERF_EVENT_ARRAY. This perf event must have the following attributes: PERF_SAMPLE_RAW as sample_type , PERF_TYPE_SOFTWARE as type , and PERF_COUNT_SW_BPF_OUTPUT as config. The <[ flags ]>(IP: 2) are used to indicate the index in <[ map ]>(IP: 1) for which the value must be put , masked with BPF_F_INDEX_MASK. Alternatively , <[ flags ]>(IP: 2) can be set to BPF_F_CURRENT_CPU to indicate that the index of the current CPU core should be used. The value to write , of <[ size ]>(IP: 4) , is passed through eBPF stack and pointed by data. The context of the program <[ ctx ]>(IP: 0) needs also be passed to the helper. On user space , a program willing to read the values needs to call perf_event_open() on the perf event (either for one or for all CPUs) and to store the file descriptor into the map. This must be done before the eBPF program can send <[ data ]>(IP: 3) into it. An example is available in file samples/bpf/trace_output_user. c in the Linux kernel source tree (the eBPF program counterpart is in samples/bpf/trace_output_kern. c). bpf_perf_event_output() achieves better performance than bpf_trace_printk() for sharing <[ data ]>(IP: 3) with user space , and is much better suitable for streaming <[ data ]>(IP: 3) from eBPF programs. Note that this helper is not restricted to tracing use cases and can be used with programs attached to TC or XDP as well , where it allows for passing <[ data ]>(IP: 3) to user space listeners. Data can be:Only custom structs ,Only the packet payload , orA combination of both. ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_perf_event_output", "Input Params": ["{Type: struct pt_reg ,Var: *ctx}", "{Type:  struct bpf_map ,Var: *map}", "{Type:  u64 ,Var: flags}", "{Type:  void ,Var: *data}", "{Type:  u64 ,Var: size}"]}},
{"bpf_skb_load_bytes": {"Description": "This helper was provided as an easy way <[ to ]>(IP: 2) load data from a packet. It can be used <[ to ]>(IP: 2) load <[ len ]>(IP: 3) bytes from <[ offset ]>(IP: 1) from the packet associated <[ to ]>(IP: 2) <[ skb ]>(IP: 0) , into the buffer pointed by to. Since Linux 4. 7 , usage of this helper has mostly been replaced by \"direct packet access\" , enabling packet data <[ to ]>(IP: 2) be manipulated with skb->data and skb->data_end pointing respectively <[ to ]>(IP: 2) the first byte of packet data and <[ to ]>(IP: 2) the byte after the last byte of packet data. However , it remains useful if one wishes <[ to ]>(IP: 2) read large quantities of data at once from a packet into the eBPF stack. ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_skb_load_bytes", "Input Params": ["{Type: const struct sk_buff ,Var: *skb}", "{Type:  u32 ,Var: offset}", "{Type:  void ,Var: *to}", "{Type:  u32 ,Var: len}"]}},
{"bpf_get_stackid": {"Description": "Walk a user or a kernel stack and return its id. To achieve this , the helper needs <[ ctx ]>(IP: 0) , which is a pointer to the context on which the tracing program is executed , and a pointer to a <[ map ]>(IP: 1) of type BPF_MAP_TYPE_STACK_TRACE. The last argument , <[ flags ]>(IP: 2) , holds the number of stack frames to skip (from 0 to 255) , masked with BPF_F_SKIP_FIELD_MASK. The next bits can be used to set a combination of the following flags:BPF_F_USER_STACKCollect a user space stack instead of a kernel stack. BPF_F_FAST_STACK_CMPCompare stacks by hash only. BPF_F_REUSE_STACKIDIf two different stacks hash into the same stackid , discard the old one. The stack id retrieved is a 32 bit long integer handle which can be further combined with other data (including other stack ids) and used as a key into maps. This can be useful for generating a variety of graphs (such as flame graphs or off-cpu graphs). For walking a stack , this helper is an improvement over bpf_probe_read() , which can be used with unrolled loops but is not efficient and consumes a lot of eBPF instructions. Instead , bpf_get_stackid() can collect up to PERF_MAX_STACK_DEPTH both kernel and user frames. Note that this limit can be controlled with the sysctl program , and that it should be manually increased in order to profile long user stacks (such as stacks for Java programs). To do so , use:\\# sysctl kernel. perf_event_max_stack=<new value> ", "Return": "The positive or null stack id on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_get_stackid", "Input Params": ["{Type: struct pt_reg ,Var: *ctx}", "{Type:  struct bpf_map ,Var: *map}", "{Type:  u64 ,Var: flags}"]}},
{"bpf_csum_diff": {"Description": "Compute a checksum difference , <[ from ]>(IP: 0) the raw buffer pointed by <[ from ]>(IP: 0) , of length <[ from_size ]>(IP: 1) (that must be a multiple of 4) , towards the raw buffer pointed by <[ to ]>(IP: 2) , of size <[ to_size ]>(IP: 3) (same remark). An optional <[ seed ]>(IP: 4) can be added <[ to ]>(IP: 2) the value (this can be cascaded , the <[ seed ]>(IP: 4) may come <[ from ]>(IP: 0) a previous call <[ to ]>(IP: 2) the helper). This is flexible enough <[ to ]>(IP: 2) be used in several ways:With <[ from_size ]>(IP: 1) == 0 , <[ to_size ]>(IP: 3) > 0 and <[ seed ]>(IP: 4) set <[ to ]>(IP: 2) checksum , it can be used when pushing new data. With <[ from_size ]>(IP: 1) > 0 , <[ to_size ]>(IP: 3) == 0 and <[ seed ]>(IP: 4) set <[ to ]>(IP: 2) checksum , it can be used when removing data <[ from ]>(IP: 0) a packet. With <[ from_size ]>(IP: 1) > 0 , <[ to_size ]>(IP: 3) > 0 and <[ seed ]>(IP: 4) set <[ to ]>(IP: 2) 0 , it can be used <[ to ]>(IP: 2) compute a diff. Note that <[ from_size ]>(IP: 1) and <[ to_size ]>(IP: 3) do not need <[ to ]>(IP: 2) be equal. This helper can be used in combination with bpf_l3_csum_replace() and bpf_l4_csum_replace() , <[ to ]>(IP: 2) which one can feed in the difference computed with bpf_csum_diff(). ", "Return": "The checksum result, or a negative error code in case of failure.", "Return Type": "s64", "Function Name": "bpf_csum_diff", "Input Params": ["{Type: __be32 ,Var: *from}", "{Type:  u32 ,Var: from_size}", "{Type:  __be32 ,Var: *to}", "{Type:  u32 ,Var: to_size}", "{Type:  __wsum ,Var: seed}"]}},
{"bpf_skb_get_tunnel_opt": {"Description": "Retrieve tunnel options metadata for the packet associated to <[ skb ]>(IP: 0) , and store the raw tunnel option data to the buffer <[ opt ]>(IP: 1) of size. This helper can be used with encapsulation devices that can operate in \"collect metadata\" mode (please refer to the related note in the description of bpf_skb_get_tunnel_key() for more details). A particular example where this can be used is in combination with the Geneve encapsulation protocol , where it allows for pushing (with bpf_skb_get_tunnel_opt() helper) and retrieving arbitrary TLVs (Type-Length-Value headers) from the eBPF program. This allows for full customization of these headers. ", "Return": "The size of the option data retrieved.", "Return Type": "int", "Function Name": "bpf_skb_get_tunnel_opt", "Input Params": ["{Type: struct sk_buff ,Var: *skb}", "{Type:  u8 ,Var: *opt}", "{Type:  u32 ,Var: size}"]}},
{"bpf_skb_set_tunnel_opt": {"Description": "Set tunnel options metadata for the packet associated to <[ skb ]>(IP: 0) to the option data contained in the raw buffer <[ opt ]>(IP: 1) of size. See also the description of the bpf_skb_get_tunnel_opt() helper for additional information. ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_skb_set_tunnel_opt", "Input Params": ["{Type: struct sk_buff ,Var: *skb}", "{Type:  u8 ,Var: *opt}", "{Type:  u32 ,Var: size}"]}},
{"bpf_skb_change_proto": {"Description": "Change the protocol of the <[ skb ]>(IP: 0) to proto. Currently supported are transition from IPv4 to IPv6 , and from IPv6 to IPv4. The helper takes care of the groundwork for the transition , including resizing the socket buffer. The eBPF program is expected to fill the new headers , if any , via skb_store_bytes() and to recompute the checksums with bpf_l3_csum_replace() and bpf_l4_csum_replace(). The main case for this helper is to perform NAT64 operations out of an eBPF program. Internally , the GSO type is marked as dodgy so that headers are checked and segments are recalculated by the GSO/GRO engine. The size for GSO target is adapted as well. All values for <[ flags ]>(IP: 2) are reserved for future usage , and must be left at zero. A call to this helper is susceptible to change the underlaying packet buffer. Therefore , at load time , all checks on pointers previously done by the verifier are invalidated and must be performed again , if the helper is used in combination with direct packet access. ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_skb_change_proto", "Input Params": ["{Type: struct sk_buff ,Var: *skb}", "{Type:  __be16 ,Var: proto}", "{Type:  u64 ,Var: flags}"]}},
{"bpf_skb_change_type": {"Description": "Change the packet <[ type ]>(IP: 1) for the packet associated to skb. This comes down to setting skb->pkt_type to <[ type ]>(IP: 1) , except the eBPF program does not have a write access to skb->pkt_type beside this helper. Using a helper here allows for graceful handling of errors. The major use case is to change incoming skb*s to **PACKET_HOST* in a programmatic way instead of having to recirculate via redirect(. . . , BPF_F_INGRESS) , for example. Note that <[ type ]>(IP: 1) only allows certain values. At this time , they are:PACKET_HOSTPacket is for us. PACKET_BROADCASTSend packet to all. PACKET_MULTICASTSend packet to group. PACKET_OTHERHOSTSend packet to someone else. ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_skb_change_type", "Input Params": ["{Type: struct sk_buff ,Var: *skb}", "{Type:  u32 ,Var: type}"]}},
{"bpf_skb_under_cgroup": {"Description": "Check whether <[ skb ]>(IP: 0) is a descendant of the cgroup2 held by <[ map ]>(IP: 1) of type BPF_MAP_TYPE_CGROUP_ARRAY , at index. ", "Return": "The return value depends on the result of the test, and can be: 0, if the skb failed the cgroup2 descendant test.1, if the skb succeeded the cgroup2 descendant test.A negative error code, if an error occurred.", "Return Type": "int", "Function Name": "bpf_skb_under_cgroup", "Input Params": ["{Type: struct sk_buff ,Var: *skb}", "{Type:  struct bpf_map ,Var: *map}", "{Type:  u32 ,Var: index}"]}},
{"bpf_get_hash_recalc": {"Description": "Retrieve the hash of the packet , skb->hash. If it is not set , in particular if the hash was cleared due to mangling , recompute this hash. Later accesses to the hash can be done directly with skb->hash. Calling bpf_set_hash_invalid() , changing a packet prototype with bpf_skb_change_proto() , or calling bpf_skb_store_bytes() with the BPF_F_INVALIDATE_HASH are actions susceptible to clear the hash and to trigger a new computation for the next call to bpf_get_hash_recalc(). ", "Return": "The 32-bit hash.", "Return Type": "u32", "Function Name": "bpf_get_hash_recalc", "Input Params": ["{Type: struct sk_buff ,Var: *skb}"]}},
{"bpf_get_current_task": {"Return": "A pointer to the current task struct.", "Return Type": "u64", "Function Name": "bpf_get_current_task", "Input Params": ["{Type: voi ,Var: void}"], "Description": "A pointer to the current task struct. "}},
{"bpf_probe_write_user": {"Description": "Attempt in a safe way to write <[ len ]>(IP: 2) bytes from the buffer <[ src ]>(IP: 1) to <[ dst ]>(IP: 0) in memory. It only works for threads that are in user context , and <[ dst ]>(IP: 0) must be a valid user space address. This helper should not be used to implement any kind of security mechanism because of TOC-TOU attacks , but rather to debug , divert , and manipulate execution of semi-cooperative processes. Keep in mind that this feature is meant for experiments , and it has a risk of crashing the system and running programs. Therefore , when an eBPF program using this helper is attached , a warning including PID and process name is printed to kernel logs. ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_probe_write_user", "Input Params": ["{Type: void ,Var: *dst}", "{Type:  const void ,Var: *src}", "{Type:  u32 ,Var: len}"]}},
{"bpf_current_task_under_cgroup": {"Description": "Check whether the probe is being run is the context of a given subset of the cgroup2 hierarchy. The cgroup2 to test is held by <[ map ]>(IP: 0) of type BPF_MAP_TYPE_CGROUP_ARRAY , at index. ", "Return": "The return value depends on the result of the test, and can be: 0, if the skb task belongs to the cgroup2.1, if the skb task does not belong to the cgroup2.A negative error code, if an error occurred.", "Return Type": "int", "Function Name": "bpf_current_task_under_cgroup", "Input Params": ["{Type: struct bpf_map ,Var: *map}", "{Type:  u32 ,Var: index}"]}},
{"bpf_skb_change_tail": {"Description": "Resize (trim or grow) the packet associated to <[ skb ]>(IP: 0) to the new len. The <[ flags ]>(IP: 2) are reserved for future usage , and must be left at zero. The basic idea is that the helper performs the needed work to change the size of the packet , then the eBPF program rewrites the rest via helpers like bpf_skb_store_bytes() , bpf_l3_csum_replace() , bpf_l3_csum_replace() and others. This helper is a slow path utility intended for replies with control messages. And because it is targeted for slow path , the helper itself can afford to be slow: it implicitly linearizes , unclones and drops offloads from the skb. A call to this helper is susceptible to change the underlaying packet buffer. Therefore , at load time , all checks on pointers previously done by the verifier are invalidated and must be performed again , if the helper is used in combination with direct packet access. ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_skb_change_tail", "Input Params": ["{Type: struct sk_buff ,Var: *skb}", "{Type:  u32 ,Var: len}", "{Type:  u64 ,Var: flags}"]}},
{"bpf_skb_pull_data": {"Description": "Pull in non-linear data in case the <[ skb ]>(IP: 0) is non-linear and not all of <[ len ]>(IP: 1) are part of the linear section. Make <[ len ]>(IP: 1) bytes from <[ skb ]>(IP: 0) readable and writable. If a zero value is passed for <[ len ]>(IP: 1) , then the whole length of the <[ skb ]>(IP: 0) is pulled. This helper is only needed for reading and writing with direct packet access. For direct packet access , testing that offsets to access are within packet boundaries (test on skb->data_end) is susceptible to fail if offsets are invalid , or if the requested data is in non-linear parts of the skb. On failure the program can just bail out , or in the case of a non-linear buffer , use a helper to make the data available. The bpf_skb_load_bytes() helper is a first solution to access the data. Another one consists in using bpf_skb_pull_data to pull in once the non-linear parts , then retesting and eventually access the data. At the same time , this also makes sure the <[ skb ]>(IP: 0) is uncloned , which is a necessary condition for direct write. As this needs to be an invariant for the write part only , the verifier detects writes and adds a prologue that is calling bpf_skb_pull_data() to effectively unclone the <[ skb ]>(IP: 0) from the very beginning in case it is indeed cloned. A call to this helper is susceptible to change the underlaying packet buffer. Therefore , at load time , all checks on pointers previously done by the verifier are invalidated and must be performed again , if the helper is used in combination with direct packet access. ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_skb_pull_data", "Input Params": ["{Type: struct sk_buff ,Var: *skb}", "{Type:  u32 ,Var: len}"]}},
{"bpf_csum_update": {"Description": "Add the checksum <[ csum ]>(IP: 1) into skb->csum in case the driver has supplied a checksum for the entire packet into that field. Return an error otherwise. This helper is intended to be used in combination with bpf_csum_diff() , in particular when the checksum needs to be updated after data has been written into the packet through direct packet access. ", "Return": "The checksum on success, or a negative error code in case of failure.", "Return Type": "s64", "Function Name": "bpf_csum_update", "Input Params": ["{Type: struct sk_buff ,Var: *skb}", "{Type:  __wsum ,Var: csum}"]}},
{"bpf_set_hash_invalid": {"Description": "Invalidate the current skb->hash. It can be used after mangling on headers through direct packet access , in order to indicate that the hash is outdated and to trigger a recalculation the next time the kernel tries to access this hash or when the bpf_get_hash_recalc() helper is called. ", "Return Type": "void", "Function Name": "bpf_set_hash_invalid", "Input Params": ["{Type: struct sk_buff ,Var: *skb}"]}},
{"bpf_get_numa_node_id": {"Description": "Return the id of the current NUMA node. The primary use case for this helper is the selection of sockets for the local NUMA node , when the program is attached to sockets using the SO_ATTACH_REUSEPORT_EBPF option (see also socket(7)) , but the helper is also available to other eBPF program types , similarly to bpf_get_smp_processor_id(). ", "Return": "The id of current NUMA node.", "Return Type": "int", "Function Name": "bpf_get_numa_node_id", "Input Params": ["{Type: voi ,Var: void}"]}},
{"bpf_skb_change_head": {"Description": "Grows headroom of packet associated to <[ skb ]>(IP: 0) and adjusts the offset of the MAC header accordingly , adding <[ len ]>(IP: 1) bytes of space. It automatically extends and reallocates memory as required. This helper can be used on a layer 3 <[ skb ]>(IP: 0) to push a MAC header for redirection into a layer 2 device. All values for <[ flags ]>(IP: 2) are reserved for future usage , and must be left at zero. A call to this helper is susceptible to change the underlaying packet buffer. Therefore , at load time , all checks on pointers previously done by the verifier are invalidated and must be performed again , if the helper is used in combination with direct packet access. ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_skb_change_head", "Input Params": ["{Type: struct sk_buff ,Var: *skb}", "{Type:  u32 ,Var: len}", "{Type:  u64 ,Var: flags}"]}},
{"bpf_xdp_adjust_head": {"Description": "Adjust (move) xdp_md->data by <[ delta ]>(IP: 1) bytes. Note that it is possible to use a negative value for delta. This helper can be used to prepare the packet for pushing or popping headers. A call to this helper is susceptible to change the underlaying packet buffer. Therefore , at load time , all checks on pointers previously done by the verifier are invalidated and must be performed again , if the helper is used in combination with direct packet access. ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_xdp_adjust_head", "Input Params": ["{Type: struct xdp_buff ,Var: *xdp_md}", "{Type:  int ,Var: delta}"]}},
{"bpf_probe_read_str": {"Description": "Copy a NUL terminated string from an unsafe address <[ unsafe_ptr ]>(IP: 2) to dst. The <[ size ]>(IP: 1) should include the terminating NUL byte. In case the string length is smaller than <[ size ]>(IP: 1) , the target is not padded with further NUL bytes. If the string length is larger than <[ size ]>(IP: 1) , just size-1 bytes are copied and the last byte is set to NUL. On success , the length of the copied string is returned. This makes this helper useful in tracing programs for reading strings , and more importantly to get its length at runtime. See the following snippet:SEC(\"kprobe/sys_open\")void bpf_sys_open(struct pt_regs *ctx){ char buf[PATHLEN ]; \\/\\/ PATHLEN is defined to 256 int res = bpf_probe_read_str(buf , sizeof(buf) , ctx->di); \\/\\/ Consume buf , for example push it to \\/\\/ userspace via bpf_perf_event_output(); we \\/\\/ can use res (the string length) as event \\/\\/ <[ size ]>(IP: 1) , after checking its boundaries. }In comparison , using bpf_probe_read() helper here instead to read the string would require to estimate the length at compile time , and would often result in copying more memory than necessary. Another useful use case is when parsing individual process arguments or individual environment variables navigating current->mm->arg_start and current->mm->env_start: using this helper and the return value , one can quickly iterate at the right offset of the memory area. ", "Return": "On success, the strictly positive length of the string, including the trailing NUL character. On error, a negative value.", "Return Type": "int", "Function Name": "bpf_probe_read_str", "Input Params": ["{Type: void ,Var: *dst}", "{Type:  int ,Var: size}", "{Type:  const void ,Var: *unsafe_ptr}"]}},
{"bpf_get_socket_cookie": {"Description": "If the struct sk_buff pointed by <[ skb ]>(IP: 0) has a known socket , retrieve the cookie (generated by the kernel) of this socket. If no cookie has been set yet , generate a new cookie. Once generated , the socket cookie remains stable for the life of the socket. This helper can be useful for monitoring per socket networking traffic statistics as it provides a unique socket identifier per namespace. ", "Return": "A 8-byte long non-decreasing number on success, or 0 if the socket field is missing inside skb.", "Return Type": "u64", "Function Name": "bpf_get_socket_cookie", "Input Params": ["{Type: struct sk_buff ,Var: *skb}"]}},
{"bpf_get_socket_uid": {"Return": "The owner UID of the socket associated to skb. If the socket is NULL, or if it is not a full socket (i.e. if it is a time-wait or a request socket instead), overflowuid value is returned (note that overflowuid might also be the actual UID value for the socket).", "Return Type": "u32", "Function Name": "bpf_get_socket_uid", "Input Params": ["{Type: struct sk_buff ,Var: *skb}"], "Description": "The owner UID of the socket associated to skb. If the socket is NULL , or if it is not a full socket (i. e. if it is a time-wait or a request socket instead) , overflowuid value is returned (note that overflowuid might also be the actual UID value for the socket). "}},
{"bpf_set_hash": {"Description": "Set the full <[ hash ]>(IP: 1) for <[ skb ]>(IP: 0) (set the field skb->hash) to value hash. ", "Return": "0", "Return Type": "u32", "Function Name": "bpf_set_hash", "Input Params": ["{Type: struct sk_buff ,Var: *skb}", "{Type:  u32 ,Var: hash}"]}},
{"bpf_setsockopt": {"Description": "Emulate a call to setsockopt() on the socket associated to <[ bpf_socket ]>(IP: 0) , which must be a full socket. The <[ level ]>(IP: 1) at which the option resides and the name <[ optname ]>(IP: 2) of the option must be specified , see setsockopt(2) for more information. The option value of length <[ optlen ]>(IP: 4) is pointed by optval. This helper actually implements a subset of setsockopt(). It supports the following levels:SOL_SOCKET , which supports the following optnames: SO_RCVBUF , SO_SNDBUF , SO_MAX_PACING_RATE , SO_PRIORITY , SO_RCVLOWAT , SO_MARK. IPPROTO_TCP , which supports the following optnames: TCP_CONGESTION , TCP_BPF_IW , TCP_BPF_SNDCWND_CLAMP. IPPROTO_IP , which supports <[ optname ]>(IP: 2) IP_TOS. IPPROTO_IPV6 , which supports <[ optname ]>(IP: 2) IPV6_TCLASS. ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_setsockopt", "Input Params": ["{Type: struct bpf_sock_ops ,Var: *bpf_socket}", "{Type:  int ,Var: level}", "{Type:  int ,Var: optname}", "{Type:  char ,Var: *optval}", "{Type:  int ,Var: optlen}"]}},
{"bpf_skb_adjust_room": {"Description": "Grow or shrink the room for data in the packet associated to <[ skb ]>(IP: 0) by <[ len_diff ]>(IP: 1) , and according to the selected mode. There is a single supported <[ mode ]>(IP: 2) at this time:BPF_ADJ_ROOM_NET: Adjust room at the network layer (room space is added or removed below the layer 3 header). All values for <[ flags ]>(IP: 3) are reserved for future usage , and must be left at zero. A call to this helper is susceptible to change the underlaying packet buffer. Therefore , at load time , all checks on pointers previously done by the verifier are invalidated and must be performed again , if the helper is used in combination with direct packet access. ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_skb_adjust_room", "Input Params": ["{Type: struct sk_buff ,Var: *skb}", "{Type:  u32 ,Var: len_diff}", "{Type:  u32 ,Var: mode}", "{Type:  u64 ,Var: flags}"]}},
{"bpf_redirect_map": {"Description": "Redirect the packet to the endpoint referenced by <[ map ]>(IP: 0) at index key. Depending on its type , this <[ map ]>(IP: 0) can contain references to net devices (for forwarding packets through other ports) , or to CPUs (for redirecting XDP frames to another CPU; but this is only implemented for native XDP (with driver support) as of this writing). All values for <[ flags ]>(IP: 2) are reserved for future usage , and must be left at zero. When used to redirect packets to net devices , this helper provides a high performance increase over bpf_redirect(). This is due to various implementation details of the underlying mechanisms , one of which is the fact that bpf_redirect_map() tries to send packet as a \"bulk\" to the device. ", "Return": "XDP_REDIRECT on success, or XDP_ABORTED on error.", "Return Type": "int", "Function Name": "bpf_redirect_map", "Input Params": ["{Type: struct bpf_map ,Var: *map}", "{Type:  u32 ,Var: key}", "{Type:  u64 ,Var: flags}"]}},
{"bpf_sk_redirect_map": {"Description": "Redirect the packet to the socket referenced by <[ map ]>(IP: 0) (of type BPF_MAP_TYPE_SOCKMAP) at index key. Both ingress and egress interfaces can be used for redirection. The BPF_F_INGRESS value in <[ flags ]>(IP: 2) is used to make the distinction (ingress path is selected if the flag is present , egress path otherwise). This is the only flag supported for now. ", "Return": "SK_PASS on success, or SK_DROP on error.", "Return Type": "int", "Function Name": "bpf_sk_redirect_map", "Input Params": ["{Type: struct bpf_map ,Var: *map}", "{Type:  u32 ,Var: key}", "{Type:  u64 ,Var: flags}"]}},
{"bpf_sock_map_update": {"Description": "Add an entry to , or update a <[ map ]>(IP: 1) referencing sockets. The <[ skops ]>(IP: 0) is used as a new value for the entry associated to key. <[ flags ]>(IP: 3) is one of:BPF_NOEXISTThe entry for <[ key ]>(IP: 2) must not exist in the map. BPF_EXISTThe entry for <[ key ]>(IP: 2) must already exist in the map. BPF_ANYNo condition on the existence of the entry for key. If the <[ map ]>(IP: 1) has eBPF programs (parser and verdict) , those will be inherited by the socket being added. If the socket is already attached to eBPF programs , this results in an error. ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_sock_map_update", "Input Params": ["{Type: struct bpf_sock_ops ,Var: *skops}", "{Type:  struct bpf_map ,Var: *map}", "{Type:  void ,Var: *key}", "{Type:  u64 ,Var: flags}"]}},
{"bpf_xdp_adjust_meta": {"Description": "Adjust the address pointed by xdp_md->data_meta by <[ delta ]>(IP: 1) (which can be positive or negative). Note that this operation modifies the address stored in xdp_md->data , so the latter must be loaded only after the helper has been called. The use of xdp_md->data_meta is optional and programs are not required to use it. The rationale is that when the packet is processed with XDP (e. g. as DoS filter) , it is possible to push further meta data along with it before passing to the stack , and to give the guarantee that an ingress eBPF program attached as a TC classifier on the same device can pick this up for further post-processing. Since TC works with socket buffers , it remains possible to set from XDP the mark or priority pointers , or other pointers for the socket buffer. Having this scratch space generic and programmable allows for more flexibility as the user is free to store whatever meta data they need. A call to this helper is susceptible to change the underlaying packet buffer. Therefore , at load time , all checks on pointers previously done by the verifier are invalidated and must be performed again , if the helper is used in combination with direct packet access. ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_xdp_adjust_meta", "Input Params": ["{Type: struct xdp_buff ,Var: *xdp_md}", "{Type:  int ,Var: delta}"]}},
{"bpf_perf_event_read_value": {"Description": "Read the value of a perf event counter , and store it into <[ buf ]>(IP: 2) of size buf_size. This helper relies on a <[ map ]>(IP: 0) of type BPF_MAP_TYPE_PERF_EVENT_ARRAY. The nature of the perf event counter is selected when <[ map ]>(IP: 0) is updated with perf event file descriptors. The <[ map ]>(IP: 0) is an array whose size is the number of available CPUs , and each cell contains a value relative to one CPU. The value to retrieve is indicated by <[ flags ]>(IP: 1) , that contains the index of the CPU to look up , masked with BPF_F_INDEX_MASK. Alternatively , <[ flags ]>(IP: 1) can be set to BPF_F_CURRENT_CPU to indicate that the value for the current CPU should be retrieved. This helper behaves in a way close to bpf_perf_event_read() helper , save that instead of just returning the value observed , it fills the <[ buf ]>(IP: 2) structure. This allows for additional data to be retrieved: in particular , the enabled and running times (in buf->enabled and buf->running , respectively) are copied. In general , bpf_perf_event_read_value() is recommended over bpf_perf_event_read() , which has some ABI issues and provides fewer functionalities. These values are interesting , because hardware PMU (Performance Monitoring Unit) counters are limited resources. When there are more PMU based perf events opened than available counters , kernel will multiplex these events so each event gets certain percentage (but not all) of the PMU time. In case that multiplexing happens , the number of samples or counter value will not reflect the case compared to when no multiplexing occurs. This makes comparison between different runs difficult. Typically , the counter value should be normalized before comparing to other experiments. The usual normalization is done as follows. normalized_counter = counter * t_enabled / t_runningWhere t_enabled is the time enabled for event and t_running is the time running for event since last normalization. The enabled and running times are accumulated since the perf event open. To achieve scaling factor between two invocations of an eBPF program , users can can use CPU id as the key (which is typical for perf array usage model) to remember the previous value and do the calculation inside the eBPF program. ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_perf_event_read_value", "Input Params": ["{Type: struct bpf_map ,Var: *map}", "{Type:  u64 ,Var: flags}", "{Type:  struct bpf_perf_event_value ,Var: *buf}", "{Type:  u32 ,Var: buf_size}"]}},
{"bpf_perf_prog_read_value": {"Description": "For en eBPF program attached to a perf event , retrieve the value of the event counter associated to <[ ctx ]>(IP: 0) and store it in the structure pointed by <[ buf ]>(IP: 1) and of size buf_size. Enabled and running times are also stored in the structure (see description of helper bpf_perf_event_read_value() for more details). ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_perf_prog_read_value", "Input Params": ["{Type: struct bpf_perf_event_data ,Var: *ctx}", "{Type:  struct bpf_perf_event_value ,Var: *buf}", "{Type:  u32 ,Var: buf_size}"]}},
{"bpf_getsockopt": {"Description": "Emulate a call to getsockopt() on the socket associated to <[ bpf_socket ]>(IP: 0) , which must be a full socket. The <[ level ]>(IP: 1) at which the option resides and the name <[ optname ]>(IP: 2) of the option must be specified , see getsockopt(2) for more information. The retrieved value is stored in the structure pointed by opval and of length optlen. This helper actually implements a subset of getsockopt(). It supports the following levels:IPPROTO_TCP , which supports <[ optname ]>(IP: 2) TCP_CONGESTION. IPPROTO_IP , which supports <[ optname ]>(IP: 2) IP_TOS. IPPROTO_IPV6 , which supports <[ optname ]>(IP: 2) IPV6_TCLASS. ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_getsockopt", "Input Params": ["{Type: struct bpf_sock_ops ,Var: *bpf_socket}", "{Type:  int ,Var: level}", "{Type:  int ,Var: optname}", "{Type:  char ,Var: *optval}", "{Type:  int ,Var: optlen}"]}},
{"bpf_override_return": {"Description": "Used for error injection , this helper uses kprobes to override the return value of the probed function , and to set it to rc. The first argument is the context <[ regs ]>(IP: 0) on which the kprobe works. This helper works by setting setting the PC (program counter) to an override function which is run in place of the original probed function. This means the probed function is not run at all. The replacement function just returns with the required value. This helper has security implications , and thus is subject to restrictions. It is only available if the kernel was compiled with the CONFIG_BPF_KPROBE_OVERRIDE configuration option , and in this case it only works on functions tagged with ALLOW_ERROR_INJECTION in the kernel code. Also , the helper is only available for the architectures having the CONFIG_FUNCTION_ERROR_INJECTION option. As of this writing , x86 architecture is the only one to support this feature. ", "Return": "0", "Return Type": "int", "Function Name": "bpf_override_return", "Input Params": ["{Type: struct pt_reg ,Var: *regs}", "{Type:  u64 ,Var: rc}"]}},
{"bpf_sock_ops_cb_flags_set": {"Description": "Attempt to set the value of the bpf_sock_ops_cb_flags field for the full TCP socket associated to bpf_sock_ops to argval. The primary use of this field is to determine if there should be calls to eBPF programs of type BPF_PROG_TYPE_SOCK_OPS at various points in the TCP code. A program of the same type can change its value , per connection and as necessary , when the connection is established. This field is directly accessible for reading , but this helper must be used for updates in order to return an error if an eBPF program tries to set a callback that is not supported in the current kernel. The supported callback values that <[ argval ]>(IP: 1) can combine are:BPF_SOCK_OPS_RTO_CB_FLAG (retransmission time out)BPF_SOCK_OPS_RETRANS_CB_FLAG (retransmission)BPF_SOCK_OPS_STATE_CB_FLAG (TCP state change)Here are some examples of where one could call such eBPF program:When RTO fires. When a packet is retransmitted. When the connection terminates. When a packet is sent. When a packet is received. ", "Return": "Code -EINVAL if the socket is not a full TCP socket; otherwise, a positive number containing the bits that could not be set is returned (which comes down to 0 if all bits were set as required).", "Return Type": "int", "Function Name": "bpf_sock_ops_cb_flags_set", "Input Params": ["{Type: struct bpf_sock_ops ,Var: *bpf_sock}", "{Type:  int ,Var: argval}"]}},
{"bpf_msg_redirect_map": {"Description": "This helper is used in programs implementing policies at the socket level. If the message <[ msg ]>(IP: 0) is allowed to pass (i. e. if the verdict eBPF program returns SK_PASS) , redirect it to the socket referenced by <[ map ]>(IP: 1) (of type BPF_MAP_TYPE_SOCKMAP) at index key. Both ingress and egress interfaces can be used for redirection. The BPF_F_INGRESS value in <[ flags ]>(IP: 3) is used to make the distinction (ingress path is selected if the flag is present , egress path otherwise). This is the only flag supported for now. ", "Return": "SK_PASS on success, or SK_DROP on error.", "Return Type": "int", "Function Name": "bpf_msg_redirect_map", "Input Params": ["{Type: struct sk_msg_buff ,Var: *msg}", "{Type:  struct bpf_map ,Var: *map}", "{Type:  u32 ,Var: key}", "{Type:  u64 ,Var: flags}"]}},
{"bpf_msg_apply_bytes": {"Description": "For socket policies , apply the verdict of the eBPF program to the next <[ bytes ]>(IP: 1) (number of bytes) of message msg. For example , this helper can be used in the following cases:A single sendmsg() or sendfile() system call contains multiple logical messages that the eBPF program is supposed to read and for which it should apply a verdict. An eBPF program only cares to read the first <[ bytes ]>(IP: 1) of a msg. If the message has a large payload , then setting up and calling the eBPF program repeatedly for all <[ bytes ]>(IP: 1) , even though the verdict is already known , would create unnecessary overhead. When called from within an eBPF program , the helper sets a counter internal to the BPF infrastructure , that is used to apply the last verdict to the next bytes. If <[ bytes ]>(IP: 1) is smaller than the current data being processed from a sendmsg() or sendfile() system call , the first <[ bytes ]>(IP: 1) will be sent and the eBPF program will be re-run with the pointer for start of data pointing to byte number <[ bytes ]>(IP: 1) + 1. If <[ bytes ]>(IP: 1) is larger than the current data being processed , then the eBPF verdict will be applied to multiple sendmsg() or sendfile() calls until <[ bytes ]>(IP: 1) are consumed. Note that if a socket closes with the internal counter holding a non-zero value , this is not a problem because data is not being buffered for <[ bytes ]>(IP: 1) and is sent as it is received. ", "Return": "0", "Return Type": "int", "Function Name": "bpf_msg_apply_bytes", "Input Params": ["{Type: struct sk_msg_buff ,Var: *msg}", "{Type:  u32 ,Var: bytes}"]}},
{"bpf_msg_cork_bytes": {"Description": "For socket policies , prevent the execution of the verdict eBPF program for message <[ msg ]>(IP: 0) until <[ bytes ]>(IP: 1) (byte number) have been accumulated. This can be used when one needs a specific number of <[ bytes ]>(IP: 1) before a verdict can be assigned , even if the data spans multiple sendmsg() or sendfile() calls. The extreme case would be a user calling sendmsg() repeatedly with 1-byte long message segments. Obviously , this is bad for performance , but it is still valid. If the eBPF program needs <[ bytes ]>(IP: 1) bytes to validate a header , this helper can be used to prevent the eBPF program to be called again until <[ bytes ]>(IP: 1) have been accumulated. ", "Return": "0", "Return Type": "int", "Function Name": "bpf_msg_cork_bytes", "Input Params": ["{Type: struct sk_msg_buff ,Var: *msg}", "{Type:  u32 ,Var: bytes}"]}},
{"bpf_msg_pull_data": {"Description": "For socket policies , pull in non-linear data from user space for <[ msg ]>(IP: 0) and set pointers msg->data and msg->data_end to <[ start ]>(IP: 1) and <[ end ]>(IP: 2) bytes offsets into <[ msg ]>(IP: 0) , respectively. If a program of type BPF_PROG_TYPE_SK_MSG is run on a <[ msg ]>(IP: 0) it can only parse data that the (data , data_end) pointers have already consumed. For sendmsg() hooks this is likely the first scatterlist element. But for calls relying on the sendpage handler (e. g. sendfile()) this will be the range (0 , 0) because the data is shared with user space and by default the objective is to avoid allowing user space to modify data while (or after) eBPF verdict is being decided. This helper can be used to pull in data and to set the <[ start ]>(IP: 1) and <[ end ]>(IP: 2) pointer to given values. Data will be copied if necessary (i. e. if data was not linear and if <[ start ]>(IP: 1) and <[ end ]>(IP: 2) pointers do not point to the same chunk). A call to this helper is susceptible to change the underlaying packet buffer. Therefore , at load time , all checks on pointers previously done by the verifier are invalidated and must be performed again , if the helper is used in combination with direct packet access. All values for <[ flags ]>(IP: 3) are reserved for future usage , and must be left at zero. ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_msg_pull_data", "Input Params": ["{Type: struct sk_msg_buff ,Var: *msg}", "{Type:  u32 ,Var: start}", "{Type:  u32 ,Var: end}", "{Type:  u64 ,Var: flags}"]}},
{"bpf_bind": {"Description": "Bind the socket associated to <[ ctx ]>(IP: 0) to the address pointed by <[ addr ]>(IP: 1) , of length addr_len. This allows for making outgoing connection from the desired IP address , which can be useful for example when all processes inside a cgroup should use one single IP address on a host that has multiple IP configured. This helper works for IPv4 and IPv6 , TCP and UDP sockets. The domain (addr->sa_family) must be AF_INET (or AF_INET6). Looking for a free port to bind to can be expensive , therefore binding to port is not permitted by the helper: addr->sin_port (or sin6_port , respectively) must be set to zero. ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_bind", "Input Params": ["{Type: struct bpf_sock_addr ,Var: *ctx}", "{Type:  struct sockaddr ,Var: *addr}", "{Type:  int ,Var: addr_len}"]}},
{"bpf_xdp_adjust_tail": {"Description": "Adjust (move) xdp_md->data_end by <[ delta ]>(IP: 1) bytes. It is only possible to shrink the packet as of this writing , therefore <[ delta ]>(IP: 1) must be a negative integer. A call to this helper is susceptible to change the underlaying packet buffer. Therefore , at load time , all checks on pointers previously done by the verifier are invalidated and must be performed again , if the helper is used in combination with direct packet access. ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_xdp_adjust_tail", "Input Params": ["{Type: struct xdp_buff ,Var: *xdp_md}", "{Type:  int ,Var: delta}"]}},
{"bpf_skb_get_xfrm_state": {"Description": "Retrieve the XFRM state (IP transform framework , see also ip-xfrm(8)) at <[ index ]>(IP: 1) in XFRM \"security path\" for skb. The retrieved value is stored in the struct bpf_xfrm_state pointed by <[ xfrm_state ]>(IP: 2) and of length size. All values for <[ flags ]>(IP: 4) are reserved for future usage , and must be left at zero. This helper is available only if the kernel was compiled with CONFIG_XFRM configuration option. ", "Return": "0 on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_skb_get_xfrm_state", "Input Params": ["{Type: struct sk_buff ,Var: *skb}", "{Type:  u32 ,Var: index}", "{Type:  struct bpf_xfrm_state ,Var: *xfrm_state}", "{Type:  u32 ,Var: size}", "{Type:  u64 ,Var: flags}"]}},
{"bpf_get_stack": {"Description": "Return a user or a kernel stack in bpf program provided buffer. To achieve this , the helper needs ctx , which is a pointer to the context on which the tracing program is executed. To store the stacktrace , the bpf program provides <[ buf ]>(IP: 1) with a nonnegative size. The last argument , <[ flags ]>(IP: 3) , holds the number of stack frames to skip (from 0 to 255) , masked with BPF_F_SKIP_FIELD_MASK. The next bits can be used to set the following flags:BPF_F_USER_STACKCollect a user space stack instead of a kernel stack. BPF_F_USER_BUILD_IDCollect buildid+offset instead of ips for user stack , only valid if BPF_F_USER_STACK is also specified. bpf_get_stack() can collect up to PERF_MAX_STACK_DEPTH both kernel and user frames , subject to sufficient large buffer size. Note that this limit can be controlled with the sysctl program , and that it should be manually increased in order to profile long user stacks (such as stacks for Java programs). To do so , use:\\# sysctl kernel. perf_event_max_stack=<new value> ", "Return": "a non-negative value equal to or less than size on success, or a negative error in case of failure.", "Return Type": "int", "Function Name": "bpf_get_stack", "Input Params": ["{Type: struct pt_regs ,Var: *regs}", "{Type:  void ,Var: *buf}", "{Type:  u32 ,Var: size}", "{Type:  u64 ,Var: flags}"]}}
]